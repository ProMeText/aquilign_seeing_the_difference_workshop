{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614a325d-51da-4a30-aac0-97029ca4feef",
   "metadata": {},
   "source": [
    "# Step 2. Using the segmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c9ef5e-da9b-4a05-917d-43cba7970541",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ee827-538a-426a-94fa-4ee46443609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join\n",
    "from transformers import BertTokenizer, AutoModelForTokenClassification\n",
    "import re\n",
    "import langid\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2878d7b-3df1-4cac-908e-1ce94a3feaf3",
   "metadata": {},
   "source": [
    "Text cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201acf9-616d-45b2-9152-56a5236e64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text: str):\n",
    "    punct = re.compile(r\"[\\.,;—:\\?!’'«»“/\\-]\")\n",
    "    cleaned_text = re.sub(punct, \"\", text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b4ba22-ce7f-4e70-8ae9-2ba14c1cba52",
   "metadata": {},
   "source": [
    "Function for tokenizing text (BERT takes a maximum of 512 tokens at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f5184d-01b5-4af4-aa30-312f9d21b383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text (BERT as a max length of 512) ; recommended : get the same length as for the training\n",
    "def tokenize(text,tokens_per_example):\n",
    "    words = text.split(\" \")\n",
    "    return [' '.join(words[i:i+tokens_per_example]) for i in range(0, len(words), tokens_per_example)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51629d0d-4f1c-40ad-a4a3-26f4ddc241fb",
   "metadata": {},
   "source": [
    "Functions to get the right labels on the right words (need to reconstitute tokens after BERT-tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ce157-7f89-4eec-9e8b-22062e1111d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the labels\n",
    "def get_labels_from_preds(preds):\n",
    "    bert_labels = []\n",
    "    for pred in preds[-1]:\n",
    "        label = [idx for idx, value in enumerate(pred) if value == max(pred)][0]\n",
    "        bert_labels.append(label)\n",
    "    return bert_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512edbb-d803-4499-9992-869d05b7cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correspondence(sent, tokenizer, verbose=False):\n",
    "    out = {}\n",
    "    tokenized_index = 0\n",
    "    for index, word in enumerate(sent):\n",
    "        # print(tokenizer.tokenize(word))\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        if verbose:\n",
    "            print(tokenized_word)\n",
    "        out[index] = tuple(item for item in range(tokenized_index, tokenized_index + len(tokenized_word)))\n",
    "        tokenized_index += len(tokenized_word)\n",
    "    human_split_to_bert = out\n",
    "    bert_split_to_human_split = {value: key for key, value in human_split_to_bert.items()}\n",
    "    return human_split_to_bert, bert_split_to_human_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b183e-f434-4107-9147-bff7de7847dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unalign_labels(human_to_bert, predicted_labels, splitted_text, verbose=False):\n",
    "    predicted_labels = predicted_labels[1:-1]\n",
    "    if verbose:\n",
    "        print(f\"Prediction: {predicted_labels}\")\n",
    "        print(human_to_bert)\n",
    "        print(splitted_text)\n",
    "    realigned_list = []\n",
    "    \n",
    "    # itering on original text\n",
    "    final_prediction = []\n",
    "    for index, value in enumerate(splitted_text):\n",
    "        predicted = human_to_bert[index]\n",
    "        # if no mismatch, copy the label\n",
    "        if len(predicted) == 1:\n",
    "            correct_label = predicted_labels[predicted[0]]\n",
    "            if verbose:\n",
    "                print(f\"Position {index}\")\n",
    "                print(predicted_labels)\n",
    "                print(predicted[0])\n",
    "                print(correct_label)\n",
    "        # mismatch\n",
    "        else:\n",
    "            correct_label = [predicted_labels[predicted[n]] for n in range(len(predicted))]\n",
    "            if verbose:\n",
    "                print(f\"predicted labels mismatch :{predicted_labels}\")\n",
    "                print(f\"len predicted mismatch {len(predicted)}\")\n",
    "                print(f\"Corresponding labels in prediction: {correct_label}\")\n",
    "            # Dans ce cas on regarde s'il y a 1 dans n'importe quelle position des rangs correspondants:\n",
    "            # on considère que BERT ne propose qu'une tokénisation plus importante que nous\n",
    "            if any([n == 1 for n in correct_label]):\n",
    "                correct_label = 1\n",
    "        final_prediction.append(correct_label)\n",
    "\n",
    "    assert len(final_prediction) == len(splitted_text), \"List mismatch\"\n",
    "\n",
    "    tokenized_sentence = \" \".join(\n",
    "        [element if final_prediction[index] != 1 else f\"\\n{element}\" for index, element in enumerate(splitted_text)])\n",
    "    if verbose:\n",
    "        print(f'final prediction {final_prediction}')\n",
    "        print(tokenized_sentence)\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576bc09b-3e54-4ce2-87e2-75c31d95eceb",
   "metadata": {},
   "source": [
    "# The segmentation function\n",
    "\n",
    "It requests:\n",
    "- the .txt file to be segmented\n",
    "- the path to the segmentation model (the one we've trained)\n",
    "- the path to the tokenization model\n",
    "- the number of tokens per example\n",
    "- the name of the folder in which the output file will be written\n",
    "- the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991629f6-06d4-4249-9a0a-374add47b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(input_file:str, \n",
    "                  model_path=None, \n",
    "                  tokenizer_name=None, \n",
    "                  remove_punct=False, \n",
    "                  tok_models:dict=None, \n",
    "                  corpus_limit=None, \n",
    "                  output_dir=None, \n",
    "                  tokens_per_example=None, \n",
    "                  device=\"cpu\", \n",
    "                  verbose=False,\n",
    "                  lang=None):\n",
    "    \"\"\"\n",
    "    Performs tokenization with given model, tokenizer on given file\n",
    "    \"\"\"\n",
    "\n",
    "    # get the file\n",
    "    with open(input_file) as f:\n",
    "        textL = f.read().splitlines()\n",
    "    localText = \" \".join(str(element) for element in textL)\n",
    "    if corpus_limit:\n",
    "        localText = localText[:round(len(localText)*corpus_limit)]\n",
    "    if remove_punct:\n",
    "        localText = remove_punctuation(localText)\n",
    "        \n",
    "    if not lang:\n",
    "        codelang, _ = langid.classify(localText[:300])\n",
    "        #  gestion des problèmes de reconnaissance de langue\n",
    "        if codelang == \"an\" or codelang == \"oc\" or codelang == \"pt\" or codelang == \"gl\":\n",
    "            codelang = \"es\"\n",
    "        if codelang == \"eo\" or codelang == \"ht\":\n",
    "            codelang = \"fr\"\n",
    "        if codelang == \"jv\":\n",
    "            codelang = \"it\"\n",
    "        print(f\"Detected lang: {codelang}\")\n",
    "    else:        \n",
    "        codelang = lang\n",
    "    \n",
    "    # get the path of the model\n",
    "    if model_path:\n",
    "        pass\n",
    "    else:\n",
    "        model_path = tok_models[codelang][\"model\"]\n",
    "        tokens_per_example = tok_models[codelang][\"tokens_per_example\"]\n",
    "        tokenizer_name = tok_models[codelang][\"tokenizer\"]\n",
    "    \n",
    "    print(f\"Using {model_path} model and {tokenizer_name} tokenizer.\")\n",
    "    new_model = AutoModelForTokenClassification.from_pretrained(model_path, num_labels=3)\n",
    "    # get the path of the default tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizer_name, max_length=tokens_per_example)\n",
    "    new_model.to(device)\n",
    "\n",
    "    # get the number of tokens per fragment to tokenize\n",
    "    if not tokens_per_example:\n",
    "        tokens_per_example = tok_models[codelang][\"tokens_per_example\"]\n",
    "    # split the full input text as slices\n",
    "    text = tokenize(localText, tokens_per_example)\n",
    "    # prepare the data\n",
    "    restruct = []\n",
    "    # apply the tok process on each slice of text\n",
    "    for i in tqdm.tqdm(text):\n",
    "        # BERT-tok\n",
    "        enco_nt_tok = tokenizer.encode(i, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        enco_nt_tok = enco_nt_tok.to(device)\n",
    "        # get the predictions from the model\n",
    "        predictions = new_model(enco_nt_tok)\n",
    "        preds = predictions[0]\n",
    "        # apply the functions\n",
    "        bert_labels = get_labels_from_preds(preds)\n",
    "        human_to_bert, bert_to_human = get_correspondence(i.split(), tokenizer)\n",
    "        new_labels = unalign_labels(human_to_bert=human_to_bert, predicted_labels=bert_labels, splitted_text=i.split())\n",
    "        tokenized = new_labels.split(\"\\n\")\n",
    "        if verbose:\n",
    "            print(i)\n",
    "            print(new_labels)\n",
    "            print(tokenized)\n",
    "        \n",
    "        \n",
    "        # Gestion du premier token.\n",
    "        try:\n",
    "            if tokenized[0] == \"\":\n",
    "                restruct.extend(tokenized[1:])\n",
    "            else:\n",
    "                last_token = restruct[-1]\n",
    "                restruct[-1] = f\"{last_token} {tokenized[0]}\"\n",
    "                restruct.extend(tokenized[1:])\n",
    "        # Pour le premier token\n",
    "        except IndexError:\n",
    "            if tokenized[0] == \"\":\n",
    "                restruct.extend(tokenized[1:])\n",
    "            else:\n",
    "                restruct.extend(tokenized)\n",
    "                \n",
    "    # On teste la non perte de tokens\n",
    "    input_text_length = len(localText.split())\n",
    "    output_text_length = len(\" \".join(restruct).split())\n",
    "    \n",
    "    assert input_text_length == input_text_length, \"Length of input text and tokenized text mismatch, something went wrong: \" \\\n",
    "                                                   f\"Input: {input_text_length}, output: {output_text_length}\"\n",
    "    print(\"No tokens were lost during the process.\")\n",
    "\n",
    "    try:\n",
    "        os.mkdir(output_dir)\n",
    "    except OSError as exception:\n",
    "        pass\n",
    "        \n",
    "    # prepare the name of the output file\n",
    "    if '/' in input_file:\n",
    "        filename_corr = input_file.rpartition('/')[-1].split('.')[0]\n",
    "    else:\n",
    "        filename_corr = input_file.split('.')[0]\n",
    "\n",
    "    output_file = join(output_dir, f'{filename_corr}-tok.txt')\n",
    "    \n",
    "\n",
    "    # write the file\n",
    "    with open(output_file, \"w\") as text_file:\n",
    "        text_file.write(\"\\n\".join(restruct))\n",
    "        print(f\"Saving to {output_file}\")\n",
    "    return restruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20113ad3-7fb8-49f5-87ab-a652c7486071",
   "metadata": {},
   "source": [
    "We are using the following parameters to segment a medieval french text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69deb20d-6af8-44bc-99ec-df4c17f77fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"ProMeText/aquilign_french_segmenter\"\n",
    "# On peut utiliser alternativement un modèle déjà publié s'il y a eu un problème lors de l'entraînement\n",
    "# model_path = \"ProMeText/aquilign_french_segmenter\"\n",
    "tokenizer_name = \"dbmdz/bert-base-french-europeana-cased\"\n",
    "remove_punct = True\n",
    "input_file= 'data/data_A_Segmenter/afEd-067.txt'\n",
    "example_length = 20\n",
    "device = 'cpu'\n",
    "# device = 'cuda:0'\n",
    "output_dir = 'data/data_apres_segm'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364835ca-692e-48bf-a47f-fdbe224535fd",
   "metadata": {},
   "source": [
    "Application of the function with arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b6573-48ad-4872-b709-0ba297a6212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_text(model_path=model_path, tokenizer_name=tokenizer_name, remove_punct=remove_punct, input_file=input_file, tokens_per_example=example_length, device=device, output_dir=output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10_env",
   "language": "python",
   "name": "3.10_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
