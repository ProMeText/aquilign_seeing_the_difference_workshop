{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d9480e-5cc9-4961-9508-6fd29f5614d3",
   "metadata": {},
   "source": [
    "## Import des librairies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c26b19b-f232-44d2-acb7-eb27763b376a",
   "metadata": {},
   "source": [
    "On commence par importer toutes les librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dbf88b-8baa-4be6-81ed-a59264280443",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'aquilign')\n",
    "import json\n",
    "import os\n",
    "\n",
    "import string\n",
    "from numpyencoder import NumpyEncoder\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "# import collatex\n",
    "import align.graph_merge as graph_merge\n",
    "import align.utils as utils\n",
    "import preproc.tok_apply as tokenize\n",
    "import preproc.syntactic_tokenization as syntactic_tokenization\n",
    "from align.encoder import Encoder\n",
    "from align.aligner import Bertalign\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b38e8a-0c68-4bf3-bb7c-b0f3cff725c6",
   "metadata": {},
   "source": [
    "On vérifie que le code de l'aligneur est bien importé:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d2a03-9916-4be7-b82d-3907f44f59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45fea2c-1e8a-4339-8453-295a00fb0969",
   "metadata": {},
   "source": [
    "## Fonction d'alignement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf74bf-7b2a-470f-8c1c-51c3ca950307",
   "metadata": {},
   "source": [
    "Cela semble marcher. Produisons la fonction globale qui permet de gérer le processus entier d'alignement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33090d50-b7c4-4dfe-912e-9fb8e54a98d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aligner:\n",
    "    \"\"\"\n",
    "    La classe Aligner initialise le moteur d'alignement, fondé sur Bertalign\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 corpus_limit:None, \n",
    "                 max_align=3, \n",
    "                 out_dir=\"out\", \n",
    "                 use_punctuation=True, \n",
    "                 input_dir=\"in\", \n",
    "                 main_wit=None, \n",
    "                 prefix=None,\n",
    "                 device=\"cpu\",\n",
    "                 tokenizer=\"regexp\", \n",
    "                 tok_models=None\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.alignment_dict = dict()\n",
    "        self.text_dict = dict()\n",
    "        self.files_path = glob.glob(f\"{input_dir}/*/*.txt\")\n",
    "        self.device = device\n",
    "        assert any([main_wit in path for path in self.files_path]), \"Main wit doesn't match witnesses paths, please check arguments. \" \\\n",
    "                                                                    f\"Main wit: {main_wit}, other wits: {self.files_path}\"\n",
    "        print(self.files_path)\n",
    "        self.main_file_index = next(index for index, path in enumerate(self.files_path) if main_wit in path)\n",
    "        self.corpus_limit = corpus_limit\n",
    "        self.max_align = max_align\n",
    "        self.out_dir = out_dir\n",
    "        self.use_punctiation = use_punctuation\n",
    "        self.prefix = prefix\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tok_models = tok_models\n",
    "        self.wit_pairs = self.create_pairs(self.files_path, self.main_file_index)\n",
    "\n",
    "        try:\n",
    "            os.mkdir(f\"result_dir\")\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        try:\n",
    "            os.mkdir(f\"result_dir/{self.out_dir}/\")\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        # Let's check the paths are correct\n",
    "        for file in self.files_path:\n",
    "            assert os.path.isfile(file), f\"Vérifier le chemin: {file}\"\n",
    "            \n",
    "\n",
    "    def parallel_align(self):\n",
    "        \"\"\"\n",
    "        This function procedes to the alignments two by two and then merges the alignments into a single alignement\n",
    "        \"\"\"\n",
    "        pivot_text = self.wit_pairs[0][0]\n",
    "        pivot_text_lang = pivot_text.split(\"/\")[-2]\n",
    "\n",
    "        # On commence par le premier texte, qui sera notre pivot\n",
    "        if self.tokenizer is None:\n",
    "            pass\n",
    "        elif self.tokenizer == \"regexp\":\n",
    "            first_tokenized_text = utils.clean_tokenized_content(\n",
    "                syntactic_tokenization.syntactic_tokenization(input_file=pivot_text, \n",
    "                                                              corpus_limit=self.corpus_limit,\n",
    "                                                              use_punctuation=True,\n",
    "                                                              lang=pivot_text_lang))\n",
    "        else:\n",
    "            first_tokenized_text = tokenize.tokenize_text(input_file=pivot_text, \n",
    "                                                          corpus_limit=self.corpus_limit, \n",
    "                                                          remove_punct=False, \n",
    "                                                          tok_models=self.tok_models, \n",
    "                                                          output_dir=self.out_dir, \n",
    "                                                          device=self.device,\n",
    "                                                          lang=pivot_text_lang)\n",
    "        \n",
    "        assert first_tokenized_text != [], \"Erreur avec le texte tokénisé du témoin base\"\n",
    "        \n",
    "        main_wit_name = self.wit_pairs[0][0].split(\"/\")[-1].split(\".\")[0]\n",
    "        utils.write_json(f\"result_dir/{self.out_dir}/tokenized_{main_wit_name}.json\", first_tokenized_text)\n",
    "        utils.write_tokenized_text(f\"result_dir/{self.out_dir}/tokenized_{main_wit_name}.txt\", first_tokenized_text)\n",
    "        \n",
    "        # We randomize the pairs. It can help resolving memory issue.\n",
    "        random.shuffle(self.wit_pairs)\n",
    "        # Puis on boucle sur chacun des autres textes\n",
    "        for index, (main_wit, wit_to_compare) in enumerate(self.wit_pairs):\n",
    "            main_wit_name = main_wit.split(\"/\")[-1].split(\".\")[0]\n",
    "            wit_to_compare_name = wit_to_compare.split(\"/\")[-1].split(\".\")[0]\n",
    "            current_wit_lang = wit_to_compare.split(\"/\")[-2]\n",
    "            print(len(first_tokenized_text))\n",
    "            if self.tokenizer is None:\n",
    "                pass\n",
    "            elif self.tokenizer == \"regexp\":\n",
    "                second_tokenized_text = utils.clean_tokenized_content(\n",
    "                    syntactic_tokenization.syntactic_tokenization(input_file=wit_to_compare, \n",
    "                                                                  corpus_limit=self.corpus_limit,\n",
    "                                                                  use_punctuation=True, \n",
    "                                                                  lang=current_wit_lang))\n",
    "            else:\n",
    "                second_tokenized_text = tokenize.tokenize_text(input_file=wit_to_compare, \n",
    "                                                               corpus_limit=self.corpus_limit,\n",
    "                                                               remove_punct=False, \n",
    "                                                               tok_models=self.tok_models,\n",
    "                                                               output_dir=self.out_dir, \n",
    "                                                               device=self.device,\n",
    "                                                               lang=current_wit_lang)\n",
    "            assert second_tokenized_text != [], f\"Erreur avec le texte tokénisé du témoin comparé {wit_to_compare_name}\"\n",
    "            utils.write_json(f\"result_dir/{self.out_dir}/tokenized_{wit_to_compare_name}.json\", second_tokenized_text)\n",
    "            utils.write_tokenized_text(f\"result_dir/{self.out_dir}/tokenized_{wit_to_compare_name}.txt\", second_tokenized_text)\n",
    "            \n",
    "            # This dict will be used to create the alignment table in csv format\n",
    "            self.text_dict[0] = first_tokenized_text\n",
    "            self.text_dict[index + 1] = second_tokenized_text\n",
    "            \n",
    "            # Let's align the texts\n",
    "            print(f\"Aligning {main_wit} with {wit_to_compare}\")\n",
    "            \n",
    "            # Tests de profil et de paramètres\n",
    "            profile = 0\n",
    "            if profile == 0:\n",
    "                margin = True\n",
    "                len_penality = True\n",
    "            else:\n",
    "                margin = False\n",
    "                len_penality = True\n",
    "            aligner = Bertalign(self.model,\n",
    "                                first_tokenized_text, \n",
    "                                second_tokenized_text, \n",
    "                                max_align= self.max_align, \n",
    "                                win=5, skip=-.2, \n",
    "                                margin=margin, \n",
    "                                len_penalty=len_penality, \n",
    "                                device=self.device)\n",
    "            aligner.align_sents()\n",
    "            \n",
    "            # We append the result to the alignment dictionnary\n",
    "            self.alignment_dict[index] = aligner.result\n",
    "            utils.write_json(f\"result_dir/{self.out_dir}/alignment_{str(index)}.json\", aligner.result)\n",
    "            utils.save_alignment_results(aligner.result, first_tokenized_text, second_tokenized_text,\n",
    "                                         f\"{main_wit_name}_{wit_to_compare_name}\", self.out_dir)\n",
    "        print(\"Done !\")\n",
    "        utils.write_json(f\"result_dir/{self.out_dir}/alignment_dict.json\", self.alignment_dict)\n",
    "\n",
    "    def create_pairs(self, full_list:list, main_wit_index:int) -> list[tuple]:\n",
    "        \"\"\"\n",
    "        From a list of witnesses and the main witness index, create all possible pairs with this witness. Returns a list \n",
    "        of tuples with the main wit and the wit to compare it to\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        main_wit = full_list.pop(int(main_wit_index))\n",
    "        for wit in full_list:\n",
    "            pairs.append((main_wit, wit))\n",
    "        return pairs\n",
    "\n",
    "    def save_final_result(self, merged_alignments:list, delimiter=\"\\t\"):\n",
    "        \"\"\"\n",
    "        Saves result to csv file\n",
    "        \"\"\"\n",
    "        \n",
    "        all_wits = [self.wit_pairs[0][0]] + [pair[1] for pair in self.wit_pairs]\n",
    "        filenames = [wit.split(\"/\")[-1].replace(\".txt\", \"\") for wit in all_wits]\n",
    "        with open(f\"result_dir/{self.out_dir}/final_result.csv\", \"w\") as output_text:\n",
    "            output_text.write(delimiter + delimiter.join(filenames) + \"\\n\")\n",
    "            # TODO: remplacer ça, c'est pas propre et ça sert à rien\n",
    "            translation_table = {letter:index for index, letter in enumerate(string.ascii_lowercase)}\n",
    "            for alignment_unit in merged_alignments:\n",
    "                output_text.write(\"|\".join(value for value in alignment_unit['a']) + delimiter)\n",
    "                for index, witness in enumerate(merged_alignments[0]):\n",
    "                    output_text.write(\"|\".join(self.text_dict[translation_table[witness]][int(value)] for value in\n",
    "                                               alignment_unit[witness]))\n",
    "                    if index + 1 != len(merged_alignments[0]):\n",
    "                        output_text.write(delimiter)\n",
    "                output_text.write(\"\\n\")\n",
    "        \n",
    "        \n",
    "        with open(f\"result_dir/{self.out_dir}/readable.csv\", \"w\") as output_text:\n",
    "            output_text.write(delimiter.join(filenames) + \"\\n\")\n",
    "            # TODO: remplacer ça, c'est pas propre et ça sert à rien\n",
    "            translation_table = {letter:index for index, letter in enumerate(string.ascii_lowercase)}\n",
    "            for alignment_unit in merged_alignments:\n",
    "                for index, witness in enumerate(merged_alignments[0]):\n",
    "                    output_text.write(\" \".join(self.text_dict[translation_table[witness]][int(value)] for value in\n",
    "                                               alignment_unit[witness]))\n",
    "                    if index + 1 != len(merged_alignments[0]):\n",
    "                        output_text.write(delimiter)\n",
    "                output_text.write(\"\\n\")\n",
    "        \n",
    "        with open(f\"result_dir/{self.out_dir}/final_result_as_index.csv\", \"w\") as output_text:\n",
    "            output_text.write(delimiter + delimiter.join(filenames) + \"\\n\")\n",
    "            for alignment_unit in merged_alignments:\n",
    "                for index, witness in enumerate(merged_alignments[0]):\n",
    "                    output_text.write(\"|\".join(value for value in\n",
    "                                               alignment_unit[witness]))\n",
    "                    if index + 1 != len(merged_alignments[0]):\n",
    "                        output_text.write(delimiter)\n",
    "                output_text.write(\"\\n\")\n",
    "\n",
    "        data = pd.read_csv(f\"result_dir/{self.out_dir}/final_result.csv\", delimiter=\"\\t\")\n",
    "        # Convert the DataFrame to an HTML table\n",
    "        html_table = data.to_html()\n",
    "        full_html_file = f\"\"\"<html>\n",
    "                          <head>\n",
    "                          <title>Alignement final</title>\n",
    "                            <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
    "                            </head>\n",
    "                          <body>\n",
    "                          {html_table}\n",
    "                          </body>\n",
    "                    </html>\"\"\"\n",
    "        with open(f\"result_dir/{self.out_dir}/final_result.html\", \"w\") as output_html:\n",
    "            output_html.write(full_html_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce918fd8-3fb5-4e76-9738-293597525d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db0bc34b-95de-4aca-8f41-65defc465637",
   "metadata": {},
   "source": [
    "## Paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174f343-1dde-4f8b-be9b-5f2a42bab65b",
   "metadata": {},
   "source": [
    "On va enfin donner les différents paramètres à l'outil pour aligner nos textes. En utilisant l'interface de ligne de commande (CLI), on aurait: `python3 main.py -o lancelot -i data/extraitsLancelot/ii-48/ -mw data/extraitsLancelot/ii-48_extrait/fr/micha-ii-48.txt -d \n",
    "cuda:0 -t bert-based`. Il faut ici directement renseigner les différents arguments (dossier de sortie, dossier d'entrée, témoin-pivot, préfixe des fichiers à produire, instrument de calcul, type de segmenteur, modèles de segmentation, utilisation de la ponctuation dans l'alignement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad91416-f251-44fe-9714-38a4f95ee335",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"lancelot\"\n",
    "input_dir = \"data/extraitsLancelot/ii-48_extrait\"\n",
    "main_wit = \"data/extraitsLancelot/ii-48_extrait/fr/micha-ii-48.txt\"\n",
    "assert input_dir != None,  \"Input dir is mandatory\"\n",
    "assert main_wit != None,  \"Main wit path is mandatory\"\n",
    "prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0842478d-3903-4f9b-8c01-db6313aab08e",
   "metadata": {},
   "source": [
    "On renseignera `cuda:0` si une carte graphique est à disposition, ce qui permet d'accélerer le traitement. Binder ne permet pas d'utiliser de carte graphique et il faut alors indiquer `cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88b9ae-ad2f-4f9c-b68c-71ec578f81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6dc66-3cfd-4c03-a7d6-ae4743855ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_limit = None\n",
    "if corpus_limit:\n",
    "    corpus_limit = float(corpus_limit)\n",
    "tokenizer = \"bert-based\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba20d6-fed8-4a7b-90bc-fb08143936c3",
   "metadata": {},
   "source": [
    "Les modèles de segmentation sont publiés sur HuggingFace, et sont directement disponibles à l'aide de la librairie Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b3e16-462f-4eb1-a8d4-81606470052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_models = {\"fr\": \n",
    "                  {\"model\": \"ProMeText/aquilign_french_segmenter\", \n",
    "                   \"tokenizer\": \"dbmdz/bert-base-french-europeana-cased\", \n",
    "                   \"tokens_per_example\": 12}, \n",
    "              \"es\": {\"model\": \"ProMeText/aquilign_spanish_segmenter\", \n",
    "                     \"tokenizer\": \"dccuchile/bert-base-spanish-wwm-cased\", \n",
    "                     \"tokens_per_example\": 30}, \n",
    "              \"it\": {\"model\": \"ProMeText/aquilign_italian_segmenter\", \n",
    "                     \"tokenizer\": \"dbmdz/bert-base-italian-xxl-cased\", \n",
    "                     \"tokens_per_example\": 12}, \n",
    "              \"la\": {\"model\": \"ProMeText/aquilign_segmenter_latin\", \n",
    "                     \"tokenizer\": \"LuisAVasquez/simple-latin-bert-uncased\", \n",
    "                     \"tokens_per_example\": 50}}\n",
    "assert tokenizer in [\"None\", \"regexp\", \"bert-based\"], \"Authorized values for tokenizer are: None, regexp, bert-based\"\n",
    "if tokenizer == \"None\":\n",
    "    tokenizer = None\n",
    "use_punctuation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99cb978-1feb-4314-a419-9dd967c0c4e4",
   "metadata": {},
   "source": [
    "On peut maintenant lancer l'outil !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a16418-3ada-40dc-9673-6167d2dba0c6",
   "metadata": {},
   "source": [
    "## Lancement de l'alignement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b7d10f-3dba-4ba8-af93-58017f2a39bf",
   "metadata": {},
   "source": [
    "On choisit d'abord le modèle de `sentence embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1e107-7689-4bc7-b59a-fc4c03b82317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model \n",
    "models = {0: \"distiluse-base-multilingual-cased-v2\", 1: \"LaBSE\", 2: \"Sonar\"}\n",
    "model = Encoder(models[int(1)], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d05911-5d47-4192-a466-bf90c593bb3c",
   "metadata": {},
   "source": [
    "Dans un deuxième temps, on initialise une instance de l'aligneur, qui imprime toutes les paires d'alignements qui vont être produites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac82bcc-3e15-4788-9d38-74cc531e8f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Punctuation for tokenization: {use_punctuation}\")\n",
    "MyAligner = Aligner(model, corpus_limit=corpus_limit, \n",
    "                    max_align=3, \n",
    "                    out_dir=out_dir, \n",
    "                    use_punctuation=use_punctuation, \n",
    "                    input_dir=input_dir, \n",
    "                    main_wit=main_wit, \n",
    "                    prefix=prefix, \n",
    "                    device=device, \n",
    "                    tokenizer=tokenizer, \n",
    "                    tok_models=tok_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f0dbf5-63eb-45e7-b5a7-f67d0a599ac2",
   "metadata": {},
   "source": [
    "On lance l'alignement parallèle: il y aura autant de résultats d'alignements que de paires de témoins. La fonction va commencer par installer les différents modèles si le notebook est lancé pour la première fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29abb482-a51f-44f0-bbf5-f575e51b28fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyAligner.parallel_align()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5197e-42e2-4c63-9b9e-b17cd8d40398",
   "metadata": {},
   "source": [
    "La classe `Aligner` produit un dictionnaire qui recense l'ensemble des alignements par paire. Il va s'agir d'enregistrer ce dictionnaire dans un fichier json, `alignment_dict.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365214bd-c0ed-48f7-ade5-1f9a57e37d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.write_json(f\"result_dir/{out_dir}/alignment_dict.json\", MyAligner.alignment_dict)\n",
    "align_dict = utils.read_json(f\"result_dir/{out_dir}/alignment_dict.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664253a4-b4c4-4f5a-8454-2e9c2492cacd",
   "metadata": {},
   "source": [
    "L'étape suivante est celle de la fusion des tables d'alignement individuelles en une seule table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353dc394-4a1e-421e-9cda-2c9849fa3b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_merged_alignments = graph_merge.merge_alignment_table(align_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dabe34d-591a-4cff-b556-98518b2e01c6",
   "metadata": {},
   "source": [
    "### Tests\n",
    "On teste les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8fa8e9-c938-4d8d-94ff-b504a2e990dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On teste si on ne perd pas de noeuds textuels\n",
    "print(\"Testing results consistency\")\n",
    "possible_witnesses = string.ascii_lowercase[:len(align_dict) + 1]\n",
    "tested_table = utils.test_tables_consistency(list_of_merged_alignments, possible_witnesses)\n",
    "# TODO: une phase de test pour voir si l'alignement final est cohérent avec les alignements deux à deux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa498b-f4c8-43ee-a8a3-3d879642ceea",
   "metadata": {},
   "source": [
    "### Production des fichiers de sortie\n",
    "On enregistre les fichiers et on produit le document HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122fe76a-6eda-419c-a981-c2ffa1833815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the final tables (indices and texts)\n",
    "MyAligner.save_final_result(merged_alignments=list_of_merged_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587e01e6-9e52-430e-969c-9e7f1b2dd011",
   "metadata": {},
   "source": [
    "Le résultat se trouve dans [result_dir/lancelot/final_result.html](result_dir/lancelot/final_result.html). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10_env",
   "language": "python",
   "name": "3.10_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
