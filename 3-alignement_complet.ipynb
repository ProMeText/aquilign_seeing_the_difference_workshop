{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a729b5f5-421d-4884-8296-148fd6b71ede",
   "metadata": {},
   "source": [
    "# Étape 3. Aligner un corpus multilingue médiéval avec Aquilign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9480e-5cc9-4961-9508-6fd29f5614d3",
   "metadata": {},
   "source": [
    "## Import des librairies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c26b19b-f232-44d2-acb7-eb27763b376a",
   "metadata": {},
   "source": [
    "On commence par importer toutes les librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24dbf88b-8baa-4be6-81ed-a59264280443",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "\n",
    "import string\n",
    "from numpyencoder import NumpyEncoder\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "# import collatex\n",
    "import aquilign.align.graph_merge as graph_merge\n",
    "import aquilign.align.utils as utils\n",
    "import aquilign.preproc.tok_apply as tokenize\n",
    "import aquilign.preproc.syntactic_tokenization as syntactic_tokenization\n",
    "from aquilign.align.encoder import Encoder\n",
    "from aquilign.align.aligner import Bertalign\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b38e8a-0c68-4bf3-bb7c-b0f3cff725c6",
   "metadata": {},
   "source": [
    "On vérifie que le code de l'aligneur est bien importé:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d2a03-9916-4be7-b82d-3907f44f59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45fea2c-1e8a-4339-8453-295a00fb0969",
   "metadata": {},
   "source": [
    "## Fonction d'alignement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf74bf-7b2a-470f-8c1c-51c3ca950307",
   "metadata": {},
   "source": [
    "Cela semble marcher. Produisons la fonction globale qui permet de gérer le processus entier d'alignement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33090d50-b7c4-4dfe-912e-9fb8e54a98d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aligner:\n",
    "    \"\"\"\n",
    "    La classe Aligner initialise le moteur d'alignement, fondé sur Bertalign\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 corpus_limit:None, \n",
    "                 max_align=3, \n",
    "                 out_dir=\"out\", \n",
    "                 use_punctuation=True, \n",
    "                 input_dir=\"in\", \n",
    "                 main_wit=None, \n",
    "                 prefix=None,\n",
    "                 device=\"cpu\",\n",
    "                 tokenizer=\"regexp\", \n",
    "                 tok_models=None\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.alignment_dict = dict()\n",
    "        self.text_dict = dict()\n",
    "        self.files_path = glob.glob(f\"{input_dir}/*/*.txt\")\n",
    "        self.device = device\n",
    "        assert any([main_wit in path for path in self.files_path]), \"Main wit doesn't match witnesses paths, please check arguments. \" \\\n",
    "                                                                    f\"Main wit: {main_wit}, other wits: {self.files_path}\"\n",
    "        print(self.files_path)\n",
    "        self.main_file_index = next(index for index, path in enumerate(self.files_path) if main_wit in path)\n",
    "        self.corpus_limit = corpus_limit\n",
    "        self.max_align = max_align\n",
    "        self.out_dir = out_dir\n",
    "        self.use_punctiation = use_punctuation\n",
    "        self.prefix = prefix\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tok_models = tok_models\n",
    "        self.wit_pairs = self.create_pairs(self.files_path, self.main_file_index)\n",
    "\n",
    "        try:\n",
    "            os.mkdir(f\"result_dir\")\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        try:\n",
    "            os.mkdir(f\"result_dir/{self.out_dir}/\")\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        # Let's check the paths are correct\n",
    "        for file in self.files_path:\n",
    "            assert os.path.isfile(file), f\"Vérifier le chemin: {file}\"\n",
    "            \n",
    "\n",
    "    def parallel_align(self):\n",
    "        \"\"\"\n",
    "        This function procedes to the alignments two by two and then merges the alignments into a single alignement\n",
    "        \"\"\"\n",
    "        pivot_text = self.wit_pairs[0][0]\n",
    "        pivot_text_lang = pivot_text.split(\"/\")[-2]\n",
    "\n",
    "        # On commence par le premier texte, qui sera notre pivot\n",
    "        if self.tokenizer is None:\n",
    "            pass\n",
    "        elif self.tokenizer == \"regexp\":\n",
    "            first_tokenized_text = utils.clean_tokenized_content(\n",
    "                syntactic_tokenization.syntactic_tokenization(input_file=pivot_text, \n",
    "                                                              corpus_limit=self.corpus_limit,\n",
    "                                                              use_punctuation=True,\n",
    "                                                              lang=pivot_text_lang))\n",
    "        else:\n",
    "            first_tokenized_text = tokenize.tokenize_text(input_file=pivot_text, \n",
    "                                                          corpus_limit=self.corpus_limit, \n",
    "                                                          remove_punct=False, \n",
    "                                                          tok_models=self.tok_models, \n",
    "                                                          output_dir=self.out_dir, \n",
    "                                                          device=self.device,\n",
    "                                                          lang=pivot_text_lang)\n",
    "        \n",
    "        assert first_tokenized_text != [], \"Erreur avec le texte tokénisé du témoin base\"\n",
    "        \n",
    "        main_wit_name = self.wit_pairs[0][0].split(\"/\")[-1].split(\".\")[0]\n",
    "        utils.write_json(f\"result_dir/{self.out_dir}/tokenized_{main_wit_name}.json\", first_tokenized_text)\n",
    "        utils.write_tokenized_text(f\"result_dir/{self.out_dir}/tokenized_{main_wit_name}.txt\", first_tokenized_text)\n",
    "        \n",
    "        # We randomize the pairs. It can help resolving memory issue.\n",
    "        random.shuffle(self.wit_pairs)\n",
    "        # Puis on boucle sur chacun des autres textes\n",
    "        for index, (main_wit, wit_to_compare) in enumerate(self.wit_pairs):\n",
    "            main_wit_name = main_wit.split(\"/\")[-1].split(\".\")[0]\n",
    "            wit_to_compare_name = wit_to_compare.split(\"/\")[-1].split(\".\")[0]\n",
    "            current_wit_lang = wit_to_compare.split(\"/\")[-2]\n",
    "            print(len(first_tokenized_text))\n",
    "            if self.tokenizer is None:\n",
    "                pass\n",
    "            elif self.tokenizer == \"regexp\":\n",
    "                second_tokenized_text = utils.clean_tokenized_content(\n",
    "                    syntactic_tokenization.syntactic_tokenization(input_file=wit_to_compare, \n",
    "                                                                  corpus_limit=self.corpus_limit,\n",
    "                                                                  use_punctuation=True, \n",
    "                                                                  lang=current_wit_lang))\n",
    "            else:\n",
    "                second_tokenized_text = tokenize.tokenize_text(input_file=wit_to_compare, \n",
    "                                                               corpus_limit=self.corpus_limit,\n",
    "                                                               remove_punct=False, \n",
    "                                                               tok_models=self.tok_models,\n",
    "                                                               output_dir=self.out_dir, \n",
    "                                                               device=self.device,\n",
    "                                                               lang=current_wit_lang)\n",
    "            assert second_tokenized_text != [], f\"Erreur avec le texte tokénisé du témoin comparé {wit_to_compare_name}\"\n",
    "            utils.write_json(f\"result_dir/{self.out_dir}/tokenized_{wit_to_compare_name}.json\", second_tokenized_text)\n",
    "            utils.write_tokenized_text(f\"result_dir/{self.out_dir}/tokenized_{wit_to_compare_name}.txt\", second_tokenized_text)\n",
    "            \n",
    "            # This dict will be used to create the alignment table in csv format\n",
    "            self.text_dict[0] = first_tokenized_text\n",
    "            self.text_dict[index + 1] = second_tokenized_text\n",
    "            \n",
    "            # Let's align the texts\n",
    "            print(f\"Aligning {main_wit} with {wit_to_compare}\")\n",
    "            \n",
    "            # Tests de profil et de paramètres\n",
    "            profile = 0\n",
    "            if profile == 0:\n",
    "                margin = True\n",
    "                len_penality = True\n",
    "            else:\n",
    "                margin = False\n",
    "                len_penality = True\n",
    "            aligner = Bertalign(self.model,\n",
    "                                first_tokenized_text, \n",
    "                                second_tokenized_text, \n",
    "                                max_align= self.max_align, \n",
    "                                win=5, skip=-.2, \n",
    "                                margin=margin, \n",
    "                                len_penalty=len_penality, \n",
    "                                device=self.device)\n",
    "            aligner.align_sents()\n",
    "            \n",
    "            # We append the result to the alignment dictionnary\n",
    "            self.alignment_dict[index] = aligner.result\n",
    "            utils.write_json(f\"result_dir/{self.out_dir}/alignment_{str(index)}.json\", aligner.result)\n",
    "            utils.save_alignment_results(aligner.result, first_tokenized_text, second_tokenized_text,\n",
    "                                         f\"{main_wit_name}_{wit_to_compare_name}\", self.out_dir)\n",
    "        print(\"Done !\")\n",
    "        utils.write_json(f\"result_dir/{self.out_dir}/alignment_dict.json\", self.alignment_dict)\n",
    "\n",
    "    def create_pairs(self, full_list:list, main_wit_index:int) -> list[tuple]:\n",
    "        \"\"\"\n",
    "        From a list of witnesses and the main witness index, create all possible pairs with this witness. Returns a list \n",
    "        of tuples with the main wit and the wit to compare it to\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        main_wit = full_list.pop(int(main_wit_index))\n",
    "        for wit in full_list:\n",
    "            pairs.append((main_wit, wit))\n",
    "        return pairs\n",
    "\n",
    "    def save_final_result(self, merged_alignments:list, delimiter=\"\\t\"):\n",
    "        \"\"\"\n",
    "        Saves result to csv file\n",
    "        \"\"\"\n",
    "        \n",
    "        all_wits = [self.wit_pairs[0][0]] + [pair[1] for pair in self.wit_pairs]\n",
    "        filenames = [wit.split(\"/\")[-1].replace(\".txt\", \"\") for wit in all_wits]\n",
    "        with open(f\"result_dir/{self.out_dir}/final_result.csv\", \"w\") as output_text:\n",
    "            output_text.write(delimiter + delimiter.join(filenames) + \"\\n\")\n",
    "            # TODO: remplacer ça, c'est pas propre et ça sert à rien\n",
    "            translation_table = {letter:index for index, letter in enumerate(string.ascii_lowercase)}\n",
    "            for alignment_unit in merged_alignments:\n",
    "                output_text.write(\"|\".join(value for value in alignment_unit['a']) + delimiter)\n",
    "                for index, witness in enumerate(merged_alignments[0]):\n",
    "                    output_text.write(\"|\".join(self.text_dict[translation_table[witness]][int(value)] for value in\n",
    "                                               alignment_unit[witness]))\n",
    "                    if index + 1 != len(merged_alignments[0]):\n",
    "                        output_text.write(delimiter)\n",
    "                output_text.write(\"\\n\")\n",
    "        \n",
    "        \n",
    "        with open(f\"result_dir/{self.out_dir}/readable.csv\", \"w\") as output_text:\n",
    "            output_text.write(delimiter.join(filenames) + \"\\n\")\n",
    "            # TODO: remplacer ça, c'est pas propre et ça sert à rien\n",
    "            translation_table = {letter:index for index, letter in enumerate(string.ascii_lowercase)}\n",
    "            for alignment_unit in merged_alignments:\n",
    "                for index, witness in enumerate(merged_alignments[0]):\n",
    "                    output_text.write(\" \".join(self.text_dict[translation_table[witness]][int(value)] for value in\n",
    "                                               alignment_unit[witness]))\n",
    "                    if index + 1 != len(merged_alignments[0]):\n",
    "                        output_text.write(delimiter)\n",
    "                output_text.write(\"\\n\")\n",
    "        \n",
    "        with open(f\"result_dir/{self.out_dir}/final_result_as_index.csv\", \"w\") as output_text:\n",
    "            output_text.write(delimiter + delimiter.join(filenames) + \"\\n\")\n",
    "            for alignment_unit in merged_alignments:\n",
    "                for index, witness in enumerate(merged_alignments[0]):\n",
    "                    output_text.write(\"|\".join(value for value in\n",
    "                                               alignment_unit[witness]))\n",
    "                    if index + 1 != len(merged_alignments[0]):\n",
    "                        output_text.write(delimiter)\n",
    "                output_text.write(\"\\n\")\n",
    "\n",
    "        data = pd.read_csv(f\"result_dir/{self.out_dir}/final_result.csv\", delimiter=\"\\t\")\n",
    "        # Convert the DataFrame to an HTML table\n",
    "        html_table = data.to_html()\n",
    "        full_html_file = f\"\"\"<html>\n",
    "                          <head>\n",
    "                          <title>Alignement final</title>\n",
    "                            <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
    "                            </head>\n",
    "                          <body>\n",
    "                          {html_table}\n",
    "                          </body>\n",
    "                    </html>\"\"\"\n",
    "        with open(f\"result_dir/{self.out_dir}/final_result.html\", \"w\") as output_html:\n",
    "            output_html.write(full_html_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce918fd8-3fb5-4e76-9738-293597525d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db0bc34b-95de-4aca-8f41-65defc465637",
   "metadata": {},
   "source": [
    "## Paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174f343-1dde-4f8b-be9b-5f2a42bab65b",
   "metadata": {},
   "source": [
    "On va enfin donner les différents paramètres à l'outil pour aligner nos textes. En utilisant l'interface de ligne de commande (CLI), on aurait: `python3 main.py -o lancelot -i data/extraitsLancelot/ii-48/ -mw data/extraitsLancelot/ii-48_extrait/fr/micha-ii-48.txt -d \n",
    "cuda:0 -t bert-based`. Il faut ici directement renseigner les différents arguments (dossier de sortie, dossier d'entrée, témoin-pivot, préfixe des fichiers à produire, instrument de calcul, type de segmenteur, modèles de segmentation, utilisation de la ponctuation dans l'alignement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad91416-f251-44fe-9714-38a4f95ee335",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"lancelot\"\n",
    "input_dir = \"data/extraitsLancelot/ii-48_extrait\"\n",
    "main_wit = \"data/extraitsLancelot/ii-48_extrait/fr/micha-ii-48.txt\"\n",
    "assert input_dir != None,  \"Input dir is mandatory\"\n",
    "assert main_wit != None,  \"Main wit path is mandatory\"\n",
    "prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0842478d-3903-4f9b-8c01-db6313aab08e",
   "metadata": {},
   "source": [
    "On renseignera `cuda:0` si une carte graphique est à disposition, ce qui permet d'accélerer le traitement. Binder ne permet pas d'utiliser de carte graphique et il faut alors indiquer `cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88b9ae-ad2f-4f9c-b68c-71ec578f81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6dc66-3cfd-4c03-a7d6-ae4743855ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_limit = None\n",
    "if corpus_limit:\n",
    "    corpus_limit = float(corpus_limit)\n",
    "tokenizer = \"bert-based\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba20d6-fed8-4a7b-90bc-fb08143936c3",
   "metadata": {},
   "source": [
    "Les modèles de segmentation sont publiés sur HuggingFace, et sont directement disponibles à l'aide de la librairie Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b3e16-462f-4eb1-a8d4-81606470052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_models = {\"fr\": \n",
    "                  {\"model\": \"ProMeText/aquilign_french_segmenter\", \n",
    "                   \"tokenizer\": \"dbmdz/bert-base-french-europeana-cased\", \n",
    "                   \"tokens_per_example\": 12}, \n",
    "              \"es\": {\"model\": \"ProMeText/aquilign_spanish_segmenter\", \n",
    "                     \"tokenizer\": \"dccuchile/bert-base-spanish-wwm-cased\", \n",
    "                     \"tokens_per_example\": 30}, \n",
    "              \"it\": {\"model\": \"ProMeText/aquilign_italian_segmenter\", \n",
    "                     \"tokenizer\": \"dbmdz/bert-base-italian-xxl-cased\", \n",
    "                     \"tokens_per_example\": 12}, \n",
    "              \"la\": {\"model\": \"ProMeText/aquilign_segmenter_latin\", \n",
    "                     \"tokenizer\": \"LuisAVasquez/simple-latin-bert-uncased\", \n",
    "                     \"tokens_per_example\": 50}}\n",
    "assert tokenizer in [\"None\", \"regexp\", \"bert-based\"], \"Authorized values for tokenizer are: None, regexp, bert-based\"\n",
    "if tokenizer == \"None\":\n",
    "    tokenizer = None\n",
    "use_punctuation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99cb978-1feb-4314-a419-9dd967c0c4e4",
   "metadata": {},
   "source": [
    "On peut maintenant lancer l'outil !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a16418-3ada-40dc-9673-6167d2dba0c6",
   "metadata": {},
   "source": [
    "## Lancement de l'alignement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b7d10f-3dba-4ba8-af93-58017f2a39bf",
   "metadata": {},
   "source": [
    "On choisit d'abord le modèle de `sentence embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1e107-7689-4bc7-b59a-fc4c03b82317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model \n",
    "models = {0: \"distiluse-base-multilingual-cased-v2\", 1: \"LaBSE\", 2: \"Sonar\"}\n",
    "model = Encoder(models[int(1)], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d05911-5d47-4192-a466-bf90c593bb3c",
   "metadata": {},
   "source": [
    "Dans un deuxième temps, on initialise une instance de l'aligneur, qui imprime toutes les paires d'alignements qui vont être produites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac82bcc-3e15-4788-9d38-74cc531e8f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Punctuation for tokenization: {use_punctuation}\")\n",
    "MyAligner = Aligner(model, corpus_limit=corpus_limit, \n",
    "                    max_align=3, \n",
    "                    out_dir=out_dir, \n",
    "                    use_punctuation=use_punctuation, \n",
    "                    input_dir=input_dir, \n",
    "                    main_wit=main_wit, \n",
    "                    prefix=prefix, \n",
    "                    device=device, \n",
    "                    tokenizer=tokenizer, \n",
    "                    tok_models=tok_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f0dbf5-63eb-45e7-b5a7-f67d0a599ac2",
   "metadata": {},
   "source": [
    "On lance l'alignement parallèle: il y aura autant de résultats d'alignements que de paires de témoins. La fonction va commencer par installer les différents modèles si le notebook est lancé pour la première fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c3f66-b07c-4b77-9405-7f7b047511ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyAligner.parallel_align()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1094d21-76a1-4e67-b233-74e367e583d5",
   "metadata": {},
   "source": [
    "On commente les différents fichiers produits: \n",
    "- `-tok.txt`: les fichiers tokénisés\n",
    "- `alignment.json`: l'alignement présenté par index sur chaque paire de textes\n",
    "- `alignment_*as_index.csv`: idem au format csv\n",
    "- `alignment*.csv`: l'alignement au format csv par paire, avec les textes en vis-à-vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5197e-42e2-4c63-9b9e-b17cd8d40398",
   "metadata": {},
   "source": [
    "La classe `Aligner` produit un dictionnaire qui recense l'ensemble des alignements par paire. Il va s'agir d'enregistrer ce dictionnaire dans un fichier json, `alignment_dict.json`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365214bd-c0ed-48f7-ade5-1f9a57e37d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.write_json(f\"result_dir/{out_dir}/alignment_dict.json\", MyAligner.alignment_dict)\n",
    "align_dict = utils.read_json(f\"result_dir/{out_dir}/alignment_dict.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937caa2-7f95-4873-be7c-a9aa955549b2",
   "metadata": {},
   "source": [
    "On peut y jeter un coup d'oeil: [result_dir/lancelot/alignment_dict.json](result_dir/lancelot/alignment_dict.json); il correspond exactement à la fusion des différents fichiers `alignment.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664253a4-b4c4-4f5a-8454-2e9c2492cacd",
   "metadata": {},
   "source": [
    "L'étape suivante est celle de la fusion des tables d'alignement individuelles en une seule table. Pour ce faire, on projette chaque unité d'alignement dans un graphe (un objet comprenant des noeuds reliés entre eux par des arêtes). Il existe un témoin en commun à tous les alignements: il suffit de relier tous les noeuds entre eux pour fusionner les tables d'alignement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353dc394-4a1e-421e-9cda-2c9849fa3b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_merged_alignments = graph_merge.merge_alignment_table(MyAligner.alignment_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dabe34d-591a-4cff-b556-98518b2e01c6",
   "metadata": {},
   "source": [
    "### Tests\n",
    "On teste les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8fa8e9-c938-4d8d-94ff-b504a2e990dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On teste si on ne perd pas de noeuds textuels\n",
    "print(\"Testing results consistency\")\n",
    "possible_witnesses = string.ascii_lowercase[:len(align_dict) + 1]\n",
    "tested_table = utils.test_tables_consistency(list_of_merged_alignments, possible_witnesses)\n",
    "# TODO: une phase de test pour voir si l'alignement final est cohérent avec les alignements deux à deux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa498b-f4c8-43ee-a8a3-3d879642ceea",
   "metadata": {},
   "source": [
    "### Production des fichiers de sortie\n",
    "On enregistre les fichiers et on produit le document HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122fe76a-6eda-419c-a981-c2ffa1833815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the final tables (indices and texts)\n",
    "MyAligner.save_final_result(merged_alignments=list_of_merged_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587e01e6-9e52-430e-969c-9e7f1b2dd011",
   "metadata": {},
   "source": [
    "Le résultat se trouve dans [result_dir/lancelot/final_result.html](result_dir/lancelot/final_result.html). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10_env",
   "language": "python",
   "name": "3.10_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
