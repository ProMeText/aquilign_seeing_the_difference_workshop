{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42cdbc2f-355c-435b-9810-19e863889587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ProMeText/aquilign_spanish_segmenter model and dccuchile/bert-base-spanish-wwm-cased tokenizer.\n",
      "3.3.1\n",
      "3.3.2\n",
      "3.3.3\n",
      "3.3.4\n",
      "3.3.5\n",
      "3.3.6\n",
      "3.3.7\n",
      "3.3.8\n",
      "3.3.9\n",
      "3.3.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:00<00:00, 49.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending <Element {http://www.tei-c.org/ns/1.0}space at 0x7f7848feacc0>\n",
      "Appending <Element {http://www.tei-c.org/ns/1.0}space at 0x7f7848feacc0>\n",
      "Test passed.\n",
      "3.3.11\n",
      "3.3.12\n",
      "3.3.13\n",
      "3.3.14\n",
      "3.3.15\n",
      "3.3.16\n",
      "3.3.17\n",
      "3.3.18\n",
      "3.3.19\n",
      "3.3.20\n",
      "3.3.21\n",
      "3.3.22\n",
      "3.3.23\n",
      "Using ProMeText/aquilign_segmenter_latin model and LuisAVasquez/simple-latin-bert-uncased tokenizer.\n",
      "3.3.1\n",
      "3.3.2\n",
      "3.3.3\n",
      "3.3.4\n",
      "3.3.5\n",
      "3.3.6\n",
      "3.3.7\n",
      "3.3.8\n",
      "3.3.9\n",
      "3.3.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                      | 0/18 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00, 32.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed.\n",
      "3.3.11\n",
      "3.3.12\n",
      "3.3.13\n",
      "3.3.14\n",
      "3.3.15\n",
      "3.3.16\n",
      "3.3.17\n",
      "3.3.18\n",
      "3.3.19\n",
      "3.3.20\n",
      "3.3.21\n",
      "3.3.22\n",
      "3.3.23\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import lxml.etree as etree\n",
    "import string\n",
    "from numpyencoder import NumpyEncoder\n",
    "import sys\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import tqdm\n",
    "import random\n",
    "# import collatex\n",
    "import aquilign.align.utils as utils\n",
    "import aquilign.preproc.syntactic_tokenization as syntactic_tokenization\n",
    "from transformers import BertTokenizer, AutoModelForTokenClassification\n",
    "from aquilign.align.encoder import Encoder\n",
    "from aquilign.align.aligner import Bertalign\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import glob\n",
    "\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        \n",
    "        pass\n",
    "\n",
    "class XMLAligner:\n",
    "\n",
    "    def __init__(self, \n",
    "                 hierarchy: list[str] = [\"tei:div[@type='livre']\", \"tei:div[@type='partie']\",\"tei:div[@type='chapitre']\"],\n",
    "                 id_attribute: str = \"n\",\n",
    "                 input_dir: str = \"\",\n",
    "                 main_wit:str = \"Rome_W\",\n",
    "                 tokenization_models:dict = {},\n",
    "                 device:str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        @param hierarchy: la hiérarchie sur laquelle boucler dans le document XML. Les documents source et cible doivent être\n",
    "        déjà alignés au niveau de la hiérarchie structurelle donnée. Sous la forme d'une liste d'expressions xpath.\n",
    "        Exemple: ['tei:div[@type='livre']', 'tei:div[@type='partie']', 'tei:div[@type='chapitre']']\n",
    "        @param id_attribute: l'attribut qui contient l'identifiant de la division minimale\n",
    "        @param witnesses: la liste des chemins vers chacun des témoins. Chaque témoin doit avoir son sigle encodée dans le xml:id\n",
    "        de la racine du fichier\n",
    "        \"\"\"\n",
    "        self.tok_models = tokenization_models\n",
    "        self.tei_namespace = 'http://www.tei-c.org/ns/1.0'\n",
    "        self.tei = '{http://www.tei-c.org/ns/1.0}'\n",
    "        self.TEINSMAP = {None: self.tei_namespace}\n",
    "        self.ns_decl = {'tei': self.tei_namespace}\n",
    "        self.hierarchy = hierarchy\n",
    "        self.id_attribute = id_attribute\n",
    "        self.input_dir = input_dir\n",
    "        self.device = device\n",
    "        self.witnesses = glob.glob(f\"{input_dir}/*.xml\")\n",
    "        self.global_text_dict = {}\n",
    "        self.main_wit = main_wit\n",
    "        if self.input_dir[-1] == \"/\":\n",
    "            self.out_dir = f\"{input_dir}out\"\n",
    "        else:\n",
    "            self.out_dir = f\"{input_dir}/out\"\n",
    "        try:\n",
    "            os.mkdir(self.out_dir)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "    \n",
    "    def align(self, number):\n",
    "        for chapter, divs in self.global_text_dict.items():\n",
    "            if number:\n",
    "                if chapter != number:\n",
    "                    continue\n",
    "            main_wit_node = divs[self.main_wit]\n",
    "            other_wit = [wit for wit in divs.keys() if wit != self.main_wit][0]\n",
    "            other_wit_node = divs[other_wit]\n",
    "            print(main_wit_node)\n",
    "            print(other_wit_node)\n",
    "            exit()\n",
    "\n",
    "    def parse_witnesses(self):\n",
    "        for witness in self.witnesses:\n",
    "            as_tree = etree.parse(witness)\n",
    "            try:\n",
    "                ID = as_tree.xpath(\"@xml:id\")[0]\n",
    "            except IndexError:\n",
    "                print(f\"Error with witness {witness}: please add sigla as xml:id\")\n",
    "            self.parsed_witnesses[ID] = as_tree\n",
    "\n",
    "    def basic_validation(self):\n",
    "        all_good = {ID:True for ID, witness in self.parsed_witnesses.items()}\n",
    "        for ID, witness in self.parsed_witnesses.items():\n",
    "            try:\n",
    "                lang = witness.xpath(\"descendant::tei:profileDesc/tei:langUsage/tei:language/@ident\", namespaces=self.ns_decl)[0]\n",
    "            except IndexError:\n",
    "                print(f\"Error with witness {ID}: please add language specification in \"\n",
    "                      f\"descendant::tei:profileDesc/tei:langUsage/tei:language/@ident\")\n",
    "                all_good[ID] = False\n",
    "        \n",
    "        if False in all_good.values():\n",
    "            print(\"Validation not passed, exiting\")\n",
    "            exit(0)\n",
    "        else:\n",
    "            print(\"All tests passed. Continuing\")\n",
    "        \n",
    "\n",
    "    def align_text(self):\n",
    "        for division in self.align_divisions():\n",
    "            pass\n",
    "\n",
    "    def align_divisions(self):\n",
    "        \"\"\"\n",
    "        Fonction qui permet de récupérer les divisions données d'un document en respectant une hiérarchie donnée.\n",
    "        Elle nourrit le dictionnaire `global_text_dict` qui contient des listes de noeuds à aligner.\n",
    "        \"\"\"\n",
    "        for wit_identifier, witness in self.parsed_witnesses.items():\n",
    "            path = \"descendant::\" + self.hierarchy\n",
    "            # On itère sur chaque niveau hiérarchique. Un système récursif devrait fonctionner mieux.\n",
    "            for minimal_division in witness.xpath(path, namespaces=self.ns_decl):\n",
    "                div_identifier = minimal_division.xpath(f\"@{self.id_attribute}\", namespaces=self.ns_decl)[0]\n",
    "                try:\n",
    "                    self.global_text_dict[div_identifier][wit_identifier] = minimal_division\n",
    "                except KeyError:\n",
    "                    self.global_text_dict[div_identifier] = {wit_identifier: minimal_division}\n",
    "        print(\"Corpus imported\")\n",
    "        \n",
    "        \n",
    "    def split_sentences(self,tokens_per_example, words):\n",
    "        as_nodes = [words[i:i + tokens_per_example] for i in range(0, len(words), tokens_per_example)]\n",
    "        as_text = [[word.xpath(\"descendant::text()\")[0] for word in sent] for sent in as_nodes]\n",
    "        return as_text\n",
    "    \n",
    "    def get_words_from_node(self, node, words_per_batch):\n",
    "        all_words, splitted = [], []\n",
    "        for paragraph in node.xpath(\"descendant::tei:p\", namespaces=self.ns_decl):\n",
    "            current_tokens_as_nodes = paragraph.xpath(\"descendant::node()[self::tei:pc|self::tei:w]\", namespaces=self.ns_decl)\n",
    "            all_words.extend(current_tokens_as_nodes)\n",
    "            all_words_text = paragraph.xpath(\"descendant::node()[self::tei:pc|self::tei:w]/text()\", namespaces=self.ns_decl)\n",
    "            splitted.extend(self.split_sentences(words_per_batch, current_tokens_as_nodes))\n",
    "        return all_words, splitted\n",
    "    \n",
    "    def bert_tokenization(self, \n",
    "                      remove_punct=False,\n",
    "                      verbose=False,\n",
    "                      codelang=None, \n",
    "                      xml_node=None,\n",
    "                      new_model=None,\n",
    "                      tokenizer=None):\n",
    "        \"\"\"\n",
    "        Performs tokenization with given model, tokenizer on given file\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "\n",
    "        # get the file\n",
    "\n",
    "        # get the number of tokens per fragment to tokenize\n",
    "        tokens_per_example = self.tok_models[codelang][\"tokens_per_example\"]\n",
    "        # split the full input text as slices\n",
    "        all_tokens, text = self.get_words_from_node(xml_node, tokens_per_example)\n",
    "        # prepare the data\n",
    "        # apply the tok process on each slice of text\n",
    "        all_delimiters = []\n",
    "        all_lenght = 0\n",
    "        actual_pos = 0\n",
    "        for idx, i in enumerate(tqdm.tqdm(text)):\n",
    "            as_string = \" \".join(i)\n",
    "            # BERT-tok\n",
    "            enco_nt_tok = tokenizer.encode(as_string, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "            enco_nt_tok = enco_nt_tok.to(device)\n",
    "            # get the predictions from the model\n",
    "            predictions = new_model(enco_nt_tok)\n",
    "            preds = predictions[0]\n",
    "            # apply the functions\n",
    "            bert_labels = get_labels_from_preds(preds)\n",
    "            human_to_bert, bert_to_human = get_correspondence(i, tokenizer)\n",
    "            delimiter_index, prediction = unalign_labels_and_get_index(human_to_bert=human_to_bert, predicted_labels=bert_labels,\n",
    "                                        splitted_text=i)\n",
    "            absolute_delimiter_index = delimiter_index\n",
    "            delimiter_index = [item + actual_pos - 1 for item in delimiter_index]\n",
    "            delimiter_index.append(len(i) - 1 + actual_pos)\n",
    "            actual_pos += len(prediction) \n",
    "            all_delimiters.extend(delimiter_index)\n",
    "            all_lenght += len(prediction)\n",
    "        # On imprime tous les délimiteurs\n",
    "        \n",
    "        # On va passer d'une liste de délimiteurs à une liste d'intervales\n",
    "        all_delimiters.insert(0, 0)\n",
    "        delims_as_intervals = [(all_delimiters[n] + 1, all_delimiters[n+1]) for n, _ in enumerate(all_delimiters[:-1])]\n",
    "        delims_as_intervals.insert(0, (0, delims_as_intervals[0][1]))\n",
    "        # delims_as_intervals.append((delims_as_intervals[-1][1] + 1, all_lenght - 1))\n",
    "        \n",
    "        # Il faut supprimer le 2e élément, le code de création des intervalles est pas bon\n",
    "        delims_as_intervals.pop(1)\n",
    "        \n",
    "        # On va créer des noeuds `tei:cl` en utilisant les informations de tokénisation. \n",
    "        # On va aussi s'occuper des noeuds informationnels autres (pb, etc). Suppose que ces\n",
    "        # noeuds ne contiennent pas d'enfants tei:w ou tei:pc.\n",
    "        for idx, (low_delim, high_delim) in enumerate(delims_as_intervals):\n",
    "            clause = etree.Element(self.tei+\"cl\", nsmap=self.TEINSMAP)\n",
    "            clause.set(\"n\", str(idx))\n",
    "            try:\n",
    "                all_tokens[high_delim].addnext(clause)\n",
    "            except IndexError:\n",
    "                print(high_delim)\n",
    "                print(len(all_tokens))\n",
    "                print(all_lenght)\n",
    "                print(\"Error\")\n",
    "                exit(0)\n",
    "            for token in all_tokens[low_delim:high_delim + 1]:\n",
    "                try:\n",
    "                    following = token.xpath(\"following-sibling::node()[1]\", namespaces=self.ns_decl)[0]\n",
    "                    if following.tag.replace(\"{http://www.tei-c.org/ns/1.0}\", \"\") not in ['w', 'pc', 'div', 'cl'] \\\n",
    "                            and len(following.xpath(\"node()[not(self::text())]\")) == 0:\n",
    "                        print(f\"Appending {following}\")\n",
    "                        clause.append(token)\n",
    "                        clause.append(following)\n",
    "                    else:\n",
    "                        clause.append(token)\n",
    "                except IndexError:\n",
    "                    print(\"Passing\")\n",
    "                    pass\n",
    "        \n",
    "        # On va tester que les clauses ne se chevauchent pas et que tous les tei:w ont une clause parent:\n",
    "        parent_test = xml_node.xpath(\"descendant::tei:p/descendant::tei:cl/tei:cl\", namespaces=self.ns_decl)\n",
    "        orphan_token_test = xml_node.xpath(\"descendant::tei:p/descendant::node()[self::tei:w or self::tei:pc][not(parent::tei:cl)]\", namespaces=self.ns_decl)\n",
    "        if len(parent_test) > 0:\n",
    "            print(\"Nested clauses, please check encoding and code\")\n",
    "        elif len(orphan_token_test) > 0:\n",
    "            print(\"Orphan token, please check encoding and code\")\n",
    "            print(etree.tostring(orphan_token_test[0]))\n",
    "        else:\n",
    "            print(\"Test passed.\")\n",
    "\n",
    "    \n",
    "    \n",
    "    def tokenize(self):\n",
    "        \"\"\"\n",
    "        Tokénisation des documents XML en mots `tei:w|tei:pc`, puis en segments `tei:cl` \n",
    "        \"\"\"\n",
    "        for file in self.witnesses:\n",
    "            command = [\"java\", \n",
    "                            \"-jar\", \n",
    "                            \"aquilign/preproc/xsl/saxon9he.jar\", \n",
    "                            \"-xi:on\", \n",
    "                            file,\n",
    "                            \"aquilign/preproc/xsl/tokenisation.xsl\", \n",
    "                            f\"output_dir={self.out_dir}\"]\n",
    "            subprocess.run(command)\n",
    "\n",
    "            regularisation = [\"java\", \"-jar\", \n",
    "                            \"aquilign/preproc/xsl/saxon9he.jar\",\n",
    "                              \"-xi:on\", \n",
    "                              file,\n",
    "                              \"aquilign/preproc/xsl/regularisation.xsl\", \n",
    "                              f\"output_dir={self.out_dir}\"]\n",
    "            subprocess.run(regularisation)\n",
    "        # for transcription_individuelle in os.listdir(\"temoins_tokenises\"):\n",
    "            # fichier_xml = f\"temoins_tokenises/{transcription_individuelle}\"\n",
    "            # self.ajout_xml_id(fichier_xml)\n",
    "        for file in glob.glob(f\"{self.out_dir}/*regularise.xml\"):\n",
    "            as_tree = etree.parse(file)\n",
    "            ID = as_tree.xpath(\"@xml:id\")\n",
    "            codelang = as_tree.xpath(\"descendant::tei:profileDesc/tei:langUsage/tei:language/@ident\", namespaces=self.ns_decl)[0]\n",
    "            model_path = self.tok_models[codelang][\"model\"]\n",
    "            tokens_per_example = self.tok_models[codelang][\"tokens_per_example\"]\n",
    "            tokenizer_name = self.tok_models[codelang][\"tokenizer\"]\n",
    "            print(f\"Using {model_path} model and {tokenizer_name} tokenizer.\")\n",
    "            new_model = AutoModelForTokenClassification.from_pretrained(model_path, num_labels=3)\n",
    "            tokenizer = BertTokenizer.from_pretrained(tokenizer_name, max_length=tokens_per_example)\n",
    "            new_model.to(self.device)\n",
    "            for chapter in as_tree.xpath(f\"descendant::{self.hierarchy.split('/')[-1]}\", namespaces=self.ns_decl):\n",
    "                print(chapter.xpath(\"@n\")[0])\n",
    "                if chapter.xpath(\"@n\")[0] != \"3.3.10\": \n",
    "                   continue\n",
    "                self.bert_tokenization(codelang=codelang, xml_node=chapter, new_model=new_model, tokenizer=tokenizer)\n",
    "            with open(f\"{self.out_dir}/{ID[0]}.phrased.xml\", \"w\") as output_file:\n",
    "                output_file.write(etree.tostring(as_tree, pretty_print=True, encoding=\"utf8\").decode('utf8'))\n",
    "        print(\"Done\")\n",
    "\n",
    "\n",
    "def unalign_labels_and_get_index(human_to_bert, predicted_labels, splitted_text, verbose=False):\n",
    "    \"\"\"\n",
    "    Réaligne les tokens BERT et les tokens humains, et produit en sortie une liste d'index de délimiteurs\n",
    "    \"\"\"\n",
    "    # On supprime SOS et EOS\n",
    "    predicted_labels = predicted_labels[1:-1]\n",
    "    if verbose:\n",
    "        print(f\"Prediction: {predicted_labels}\")\n",
    "        print(human_to_bert)\n",
    "        print(splitted_text)\n",
    "    realigned_list = []\n",
    "\n",
    "    # itering on original text\n",
    "    final_prediction = []\n",
    "    any_one = []\n",
    "    for index, value in enumerate(splitted_text):\n",
    "        predicted = human_to_bert[index]\n",
    "        # if no mismatch, copy the label\n",
    "        if len(predicted) == 1:\n",
    "            correct_label = predicted_labels[predicted[0]]\n",
    "            if verbose:\n",
    "                pass\n",
    "                # print(f\"Position {index}\")\n",
    "                # print(predicted_labels)\n",
    "                # print(predicted[0])\n",
    "                # print(correct_label)\n",
    "        # mismatch\n",
    "        else:\n",
    "            correct_label = [predicted_labels[predicted[n]] for n in range(len(predicted))]\n",
    "            if verbose:\n",
    "                print(f\"predicted labels mismatch :{predicted_labels}\")\n",
    "                print(f\"len predicted mismatch {len(predicted)}\")\n",
    "                print(f\"Corresponding labels in prediction: {correct_label}\")\n",
    "            # Dans ce cas on regarde s'il y a 1 dans n'importe quelle position des rangs correspondants:\n",
    "            # on considère que BERT ne propose qu'une tokénisation plus granulaire que nous\n",
    "            if any([n == 1 for n in correct_label]):\n",
    "                correct_label = 1\n",
    "            else:\n",
    "                correct_label = 0\n",
    "        final_prediction.append(correct_label)\n",
    "        \n",
    "    assert len(final_prediction) == len(splitted_text), \"List mismatch\"\n",
    "\n",
    "    # On récupère les lieux où on doit couper\n",
    "    index_list = [idx for idx, token in enumerate(final_prediction) if\n",
    "                  token == 1 or (isinstance(token, list) and 1 in token)]\n",
    "    return index_list, final_prediction\n",
    "\n",
    "\n",
    "# correspondences between our labels and labels from the BERT-tok\n",
    "def get_correspondence(sent, tokenizer, verbose=False):\n",
    "    out = {}\n",
    "    tokenized_index = 0\n",
    "    for index, word in enumerate(sent):\n",
    "        # print(tokenizer.tokenize(word))\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        if verbose:\n",
    "            print(tokenized_word)\n",
    "        out[index] = tuple(item for item in range(tokenized_index, tokenized_index + len(tokenized_word)))\n",
    "        tokenized_index += len(tokenized_word)\n",
    "    human_split_to_bert = out\n",
    "    bert_split_to_human_split = {value: key for key, value in human_split_to_bert.items()}\n",
    "    return human_split_to_bert, bert_split_to_human_split\n",
    "\n",
    "#get the labels\n",
    "def get_labels_from_preds(preds):\n",
    "    bert_labels = []\n",
    "    for pred in preds[-1]:\n",
    "        label = [idx for idx, value in enumerate(pred) if value == max(pred)][0]\n",
    "        bert_labels.append(label)\n",
    "    return bert_labels\n",
    "\n",
    "\n",
    "def main(input_dir, main_wit, hierarchy, id_attribute, tokenization_models, device):\n",
    "    TEIAligner = XMLAligner(input_dir=input_dir,\n",
    "                            hierarchy=hierarchy,\n",
    "                            main_wit=main_wit,\n",
    "                            id_attribute=id_attribute,\n",
    "                            tokenization_models=tokenization_models,\n",
    "                            device=device)\n",
    "\n",
    "\n",
    "    TEIAligner.tokenize()\n",
    "    # On réécrit la liste des témoins pour aller chercher dans les fichers de sortie\n",
    "    # TEIAligner.witnesses = glob.glob(f\"{TEIAligner.out_dir}/*.xml\")\n",
    "    # TEIAligner.global_text_dict = {}\n",
    "    # TEIAligner.parsed_witnesses = {}\n",
    "    # TEIAligner.parse_witnesses()\n",
    "    # TEIAligner.basic_validation()\n",
    "    # TEIAligner.align_divisions()\n",
    "    # print(TEIAligner.global_text_dict)\n",
    "    # TEIAligner.align(number=\"3.3.10\")\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    attribute = \"n\"\n",
    "    hierarchy =  \"tei:div[@type='livre']/tei:div[@type='partie']/tei:div[@type='chapitre']\"\n",
    "    input_dir = \"data/XML_test/\"\n",
    "    main_wit = \"Val_S\"\n",
    "    device = \"cuda:0\"\n",
    "    corpus_limit = None\n",
    "    if corpus_limit:\n",
    "        corpus_limit = float(corpus_limit)\n",
    "    tokenizer = \"bert-based\"\n",
    "    tokenization_models = {\"fr\":\n",
    "                      {\"model\": \"ProMeText/aquilign_french_segmenter\",\n",
    "                       \"tokenizer\": \"dbmdz/bert-base-french-europeana-cased\",\n",
    "                       \"tokens_per_example\": 12},\n",
    "                  \"es\": {\"model\": \"ProMeText/aquilign_spanish_segmenter\",\n",
    "                         \"tokenizer\": \"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "                         \"tokens_per_example\": 30},\n",
    "                  \"it\": {\"model\": \"ProMeText/aquilign_italian_segmenter\",\n",
    "                         \"tokenizer\": \"dbmdz/bert-base-italian-xxl-cased\",\n",
    "                         \"tokens_per_example\": 12},\n",
    "                  \"la\": {\"model\": \"ProMeText/aquilign_segmenter_latin\",\n",
    "                         \"tokenizer\": \"LuisAVasquez/simple-latin-bert-uncased\",\n",
    "                         \"tokens_per_example\": 50}}\n",
    "    \n",
    "    assert tokenizer in [\"None\", \"regexp\",\n",
    "                         \"bert-based\"], \"Authorized values for tokenizer are: None, regexp, bert-based\"\n",
    "    assert input_dir != None, \"Input dir is mandatory\"\n",
    "    \n",
    "    \n",
    "    main(input_dir, main_wit, hierarchy, attribute, tokenization_models, device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383298fe-c294-416e-bbdc-e6960de510b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10_env",
   "language": "python",
   "name": "3.10_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
