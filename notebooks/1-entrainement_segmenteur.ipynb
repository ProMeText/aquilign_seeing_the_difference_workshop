{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e710e2-b398-47ba-a89c-6124cfb65b76",
   "metadata": {},
   "source": [
    "# Étape 1. Entraînement d'un segmenteur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681c4bb-d184-4c82-aeeb-3f1874372305",
   "metadata": {},
   "source": [
    "Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4b821-dad7-4a45-91de-8346ae4d6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import sys\n",
    "from transformers import BertTokenizer, Trainer, TrainingArguments, AutoModelForTokenClassification, set_seed\n",
    "sys.path.insert(1, '../aquilign')\n",
    "import preproc.tok_trainer_functions as trainer_functions\n",
    "import preproc.eval as evaluation\n",
    "import preproc.utils as utils\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import argparse\n",
    "#shutil usefull for deleting not empty directories \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537553f-ad79-4d32-85d0-fab2d57b1836",
   "metadata": {},
   "source": [
    "L'exécution du script permet d'entraîner un modèle de segmentation automatique de texte. Trois fichiers doivent être fournis, tous au format spécifié (chaque token devant être identifié comme segmentant le texte doit être précédé d'un '£') : un fichier contenant les données d'entraînement, un contenant les données de dev, et un dernier contenant les données de test. Les fichiers doivent être dans un dossier contenant le code ISO de la langue dans laquelle ils sont écrits (ce code est récupéré au moment de l'évaluation des modèles).\n",
    "\n",
    "Le meilleur modèle est enregistré à la fin de l'entraînement. L'évaluation se base à la fois sur la loss, et sur les métriques plus classiques d'évaluation. Dans notre script, c'est la précision qui prend le poids le plus important.\n",
    "L'évaluation passe également par l'évaluation comparée d'une segmentation basée sur des regex. Il est donc nécessaire de remplir le fichier delimiters.json avec des exemple de regex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77799585-3a43-4343-bdab-24f61cc08628",
   "metadata": {},
   "outputs": [],
   "source": [
    "## command line usage : python tok_trainer.py model_name tok_name train_file.txt dev_file.txt num_train_epochs batch_size logging_steps\n",
    "## where :\n",
    "# model_name is the full name of the model (same name for model and tokenizer or not)\n",
    "# tok_name is the full name of the tokenizer (can be the same)\n",
    "# train_file.txt is the file with the sentences and words of interest are identified  (words are identified with £ in the line)\n",
    "# which will be used for training\n",
    "## ex. : uoulentiers £mais il nen est pas encor temps. £Certes fait elle\n",
    "# dev_file.txt is the file with the sentences and words of interest which will be used for eval\n",
    "# num_train_epochs : the number of epochs we want to train (ex : 10)\n",
    "# batch_size : the batch size (ex : 8)\n",
    "# logging_steps : the number of logging steps (ex : 50)\n",
    "\n",
    "## was changed : if you want to fine-tune a model, we need to have two different names for model_name and tok_name (can also be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455a345-a250-4340-baee-d5a9cd9b49b0",
   "metadata": {},
   "source": [
    "La fonction training_trainer prend plusieurs arguments :\n",
    "- model_name : le nom du modèle AutoModelForTokenClassification\n",
    "- tok_name : le nom du modèle BertTokenizer\n",
    "(ces deux noms peuvent éventuellement être les mêmes, si l'on ne fine-tune pas un modèle spécifique)\n",
    "- train_dataset : le chemin du fichier des données d'entraînement\n",
    "- dev_dataset : le chemin du fichier des données de dev\n",
    "- eval_dataset : le chemin du fichier des données de test\n",
    "- num_train_epochs : le nombre d'époques d'entraînement (min. 2)\n",
    "- batch_size\n",
    "- logging_steps\n",
    "  \n",
    "Et en plus, un argument permettant de dire si on veut aussi garder la ponctuation ou non comme aide à la segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba8ae12-6c46-49be-a526-c53b6d0ede10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_trainer(model_name, tok_name, train_dataset, dev_dataset, eval_dataset, num_train_epochs, batch_size, logging_steps, keep_punct=True):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=3)\n",
    "    tokenizer = BertTokenizer.from_pretrained(tok_name, max_length=10)\n",
    "    \n",
    "    with open(train_dataset, \"r\") as train_file:\n",
    "        train_lines = [item.replace(\"\\n\", \"\") for item in train_file.readlines()]\n",
    "        if keep_punct is False:\n",
    "            train_lines = [utils.remove_punctuation(line) for line in train_lines]\n",
    "        \n",
    "    with open(dev_dataset, \"r\") as dev_file:\n",
    "        dev_lines = [item.replace(\"\\n\", \"\") for item in dev_file.readlines()]\n",
    "        if keep_punct is False:\n",
    "            dev_lines = [utils.remove_punctuation(line) for line in dev_lines]\n",
    "        \n",
    "    with open(eval_dataset, \"r\") as eval_files:\n",
    "        eval_lines = [item.replace(\"\\n\", \"\") for item in eval_files.readlines()]\n",
    "    eval_data_lang = eval_dataset.split(\"/\")[-2]\n",
    "    \n",
    "    # Train corpus\n",
    "    train_texts_and_labels = utils.convertToSubWordsSentencesAndLabels(train_lines, tokenizer=tokenizer, delimiter=\"£\")\n",
    "    train_dataset = trainer_functions.SentenceBoundaryDataset(train_texts_and_labels, tokenizer)\n",
    "    \n",
    "    # Dev corpus\n",
    "    dev_texts_and_labels = utils.convertToSubWordsSentencesAndLabels(dev_lines, tokenizer=tokenizer, delimiter=\"£\")\n",
    "    dev_dataset = trainer_functions.SentenceBoundaryDataset(dev_texts_and_labels, tokenizer)\n",
    "\n",
    "    if '/' in model_name:\n",
    "        name_of_model = re.split('/', model_name)[1]\n",
    "    else:\n",
    "        name_of_model = model_name\n",
    "\n",
    "    # training arguments\n",
    "    # num train epochs, logging_steps and batch_size should be provided\n",
    "    # evaluation is done by epoch and the best model of each one is stored in a folder \"results_+name\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"results_{name_of_model}/epoch{num_train_epochs}_bs{batch_size}\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        logging_steps=logging_steps,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        dataloader_num_workers=8,\n",
    "        dataloader_prefetch_factor=4,\n",
    "        # ajout pour résoudre pb : save_safetensors= False et bf16=False\n",
    "        bf16=False,    \n",
    "        save_safetensors=False,\n",
    "        #modif : cpu\n",
    "        use_cpu=True,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True\n",
    "        # best model is evaluated on loss\n",
    "    )\n",
    "\n",
    "    # define the trainer : model, training args, datasets and the specific compute_metrics defined in functions file\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        compute_metrics=trainer_functions.compute_metrics\n",
    "    )\n",
    "\n",
    "    # fine-tune the model\n",
    "    print(\"Starting training\")\n",
    "    trainer.train()\n",
    "    print(\"End of training\")\n",
    "\n",
    "    # get the best model path\n",
    "    best_model_path = trainer.state.best_model_checkpoint\n",
    "    print(best_model_path)\n",
    "    print(f\"Evaluation.\")\n",
    "    \n",
    "    \n",
    "    # print the whole log_history with the compute metrics\n",
    "    best_precision_step, best_step_metrics = utils.get_best_step(trainer.state.log_history)\n",
    "    best_model_path = f\"results_{name_of_model}/epoch{num_train_epochs}_bs{batch_size}/checkpoint-{best_precision_step}\"\n",
    "    print(f\"Best model path according to precision: {best_model_path}\")\n",
    "    print(f\"Full metrics: {best_step_metrics}\")\n",
    "    \n",
    "    eval_results = evaluation.run_eval(data=eval_lines, \n",
    "                        model_path=best_model_path, \n",
    "                        tokenizer_name=tokenizer.name_or_path, \n",
    "                        verbose=False, \n",
    "                        lang=eval_data_lang)\n",
    "    \n",
    "\n",
    "    # We move the best state dir name to \"best\"\n",
    "    new_best_path = f\"results_{name_of_model}/epoch{num_train_epochs}_bs{batch_size}/best\"\n",
    "    try:\n",
    "        #os.rmdir(new_best_path)\n",
    "        shutil.rmtree(new_best_path)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    os.rename(best_model_path, new_best_path)\n",
    "    \n",
    "    #with open(f\"{new_best_path}/model_name\", \"w\") as model_name:\n",
    "    #    model_name.write(modelName)\n",
    "\n",
    "    with open(f\"{new_best_path}/eval.txt\", \"w\") as evaluation_results:\n",
    "        evaluation_results.write(eval_results)\n",
    "\n",
    "    with open(f\"{new_best_path}/metrics.json\", \"w\") as metrics:\n",
    "        json.dump(best_step_metrics, metrics)\n",
    "    \n",
    "    print(f\"\\n\\nBest model can be found at : {new_best_path} \")\n",
    "    print(f\"You should remove the following directories by using `rm -r results_{name_of_model}/epoch{num_train_epochs}_bs{batch_size}/checkpoint-*`\")\n",
    "\n",
    "    # functions returns best model_path\n",
    "    return new_best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d9d75-163a-4e61-a1bf-ef8a32417db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_trainer('dbmdz/bert-base-french-europeana-cased', 'dbmdz/bert-base-french-europeana-cased', '../data/data_to_segmenter/fr/randomSentencesComplete-gf.txt', '../data/data_to_segmenter/fr/randomSentencesEvalComplete-gf.txt', '../data/data_to_segmenter/fr/randomSentencesfrancais-pourtest-complete-gf.txt', 2, 8, 50, keep_punct=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10_env",
   "language": "python",
   "name": "3.10_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
