{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e710e2-b398-47ba-a89c-6124cfb65b76",
   "metadata": {},
   "source": [
    "### Notebook pour l'entra√Ænement d'un segmenteur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681c4bb-d184-4c82-aeeb-3f1874372305",
   "metadata": {},
   "source": [
    "Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abb4b821-dad7-4a45-91de-8346ae4d6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import sys\n",
    "from transformers import BertTokenizer, Trainer, TrainingArguments, AutoModelForTokenClassification, set_seed\n",
    "import aquilign.preproc.tok_trainer_functions as trainer_functions\n",
    "import aquilign.preproc.eval as evaluation\n",
    "import aquilign.preproc.utils as utils\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import argparse\n",
    "#shutil usefull for deleting not empty directories \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537553f-ad79-4d32-85d0-fab2d57b1836",
   "metadata": {},
   "source": [
    "L'ex√©cution du script permet d'entra√Æner un mod√®le de segmentation automatique de texte. Trois fichiers doivent √™tre fournis, tous au format sp√©cifi√© (chaque token devant √™tre identifi√© comme segmentant le texte doit √™tre pr√©c√©d√© d'un '¬£') : un fichier contenant les donn√©es d'entra√Ænement, un contenant les donn√©es de dev, et un dernier contenant les donn√©es de test. Les fichiers doivent √™tre dans un dossier contenant le code ISO de la langue dans laquelle ils sont √©crits (ce code est r√©cup√©r√© au moment de l'√©valuation des mod√®les).\n",
    "\n",
    "Le meilleur mod√®le est enregistr√© √† la fin de l'entra√Ænement. L'√©valuation se base √† la fois sur la loss, et sur les m√©triques plus classiques d'√©valuation. Dans notre script, c'est la pr√©cision qui prend le poids le plus important.\n",
    "L'√©valuation passe √©galement par l'√©valuation compar√©e d'une segmentation bas√©e sur des regex. Il est donc n√©cessaire de remplir le fichier delimiters.json avec des exemple de regex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77799585-3a43-4343-bdab-24f61cc08628",
   "metadata": {},
   "outputs": [],
   "source": [
    "## command line usage : python tok_trainer.py model_name tok_name train_file.txt dev_file.txt num_train_epochs batch_size logging_steps\n",
    "## where :\n",
    "# model_name is the full name of the model (same name for model and tokenizer or not)\n",
    "# tok_name is the full name of the tokenizer (can be the same)\n",
    "# train_file.txt is the file with the sentences and words of interest are identified  (words are identified with ¬£ in the line)\n",
    "# which will be used for training\n",
    "## ex. : uoulentiers ¬£mais il nen est pas encor temps. ¬£Certes fait elle\n",
    "# dev_file.txt is the file with the sentences and words of interest which will be used for eval\n",
    "# num_train_epochs : the number of epochs we want to train (ex : 10)\n",
    "# batch_size : the batch size (ex : 8)\n",
    "# logging_steps : the number of logging steps (ex : 50)\n",
    "\n",
    "## was changed : if you want to fine-tune a model, we need to have two different names for model_name and tok_name (can also be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455a345-a250-4340-baee-d5a9cd9b49b0",
   "metadata": {},
   "source": [
    "La fonction training_trainer prend plusieurs arguments :\n",
    "- model_name : le nom du mod√®le AutoModelForTokenClassification\n",
    "- tok_name : le nom du mod√®le BertTokenizer\n",
    "(ces deux noms peuvent √©ventuellement √™tre les m√™mes, si l'on ne fine-tune pas un mod√®le sp√©cifique)\n",
    "- train_dataset : le chemin du fichier des donn√©es d'entra√Ænement\n",
    "- dev_dataset : le chemin du fichier des donn√©es de dev\n",
    "- eval_dataset : le chemin du fichier des donn√©es de test\n",
    "- num_train_epochs : le nombre d'√©poques d'entra√Ænement (min. 2)\n",
    "- batch_size\n",
    "- logging_steps\n",
    "  \n",
    "Et en plus, un argument permettant de dire si on veut aussi garder la ponctuation ou non comme aide √† la segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ba8ae12-6c46-49be-a526-c53b6d0ede10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_trainer(model_name, tok_name, train_dataset, dev_dataset, eval_dataset, num_train_epochs, batch_size, logging_steps, keep_punct=True):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=3)\n",
    "    tokenizer = BertTokenizer.from_pretrained(tok_name, max_length=10)\n",
    "    \n",
    "    with open(train_dataset, \"r\") as train_file:\n",
    "        train_lines = [item.replace(\"\\n\", \"\") for item in train_file.readlines()]\n",
    "        if keep_punct is False:\n",
    "            train_lines = [utils.remove_punctuation(line) for line in train_lines]\n",
    "        \n",
    "    with open(dev_dataset, \"r\") as dev_file:\n",
    "        dev_lines = [item.replace(\"\\n\", \"\") for item in dev_file.readlines()]\n",
    "        if keep_punct is False:\n",
    "            dev_lines = [utils.remove_punctuation(line) for line in dev_lines]\n",
    "        \n",
    "    with open(eval_dataset, \"r\") as eval_files:\n",
    "        eval_lines = [item.replace(\"\\n\", \"\") for item in eval_files.readlines()]\n",
    "    eval_data_lang = eval_dataset.split(\"/\")[-2]\n",
    "    \n",
    "    # Train corpus\n",
    "    train_texts_and_labels = utils.convertToSubWordsSentencesAndLabels(train_lines, tokenizer=tokenizer, delimiter=\"¬£\")\n",
    "    train_dataset = trainer_functions.SentenceBoundaryDataset(train_texts_and_labels, tokenizer)\n",
    "    \n",
    "    # Dev corpus\n",
    "    dev_texts_and_labels = utils.convertToSubWordsSentencesAndLabels(dev_lines, tokenizer=tokenizer, delimiter=\"¬£\")\n",
    "    dev_dataset = trainer_functions.SentenceBoundaryDataset(dev_texts_and_labels, tokenizer)\n",
    "\n",
    "    if '/' in model_name:\n",
    "        name_of_model = re.split('/', model_name)[1]\n",
    "    else:\n",
    "        name_of_model = model_name\n",
    "\n",
    "    # training arguments\n",
    "    # num train epochs, logging_steps and batch_size should be provided\n",
    "    # evaluation is done by epoch and the best model of each one is stored in a folder \"results_+name\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"results_{name_of_model}/epoch{num_train_epochs}_bs{batch_size}\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        logging_steps=logging_steps,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        dataloader_num_workers=8,\n",
    "        dataloader_prefetch_factor=4,\n",
    "        # ajout pour r√©soudre pb : save_safetensors= False et bf16=False\n",
    "        bf16=False,    \n",
    "        save_safetensors=False,\n",
    "        #modif : cpu\n",
    "        use_cpu=True,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True\n",
    "        # best model is evaluated on loss\n",
    "    )\n",
    "\n",
    "    # define the trainer : model, training args, datasets and the specific compute_metrics defined in functions file\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        compute_metrics=trainer_functions.compute_metrics\n",
    "    )\n",
    "\n",
    "    # fine-tune the model\n",
    "    print(\"Starting training\")\n",
    "    trainer.train()\n",
    "    print(\"End of training\")\n",
    "\n",
    "    # get the best model path\n",
    "    best_model_path = trainer.state.best_model_checkpoint\n",
    "    print(best_model_path)\n",
    "    print(f\"Evaluation.\")\n",
    "    \n",
    "    \n",
    "    # print the whole log_history with the compute metrics\n",
    "    best_precision_step, best_step_metrics = utils.get_best_step(trainer.state.log_history)\n",
    "    best_model_path = f\"results_{name_of_model}/epoch{num_train_epochs}_bs{batch_size}/checkpoint-{best_precision_step}\"\n",
    "    print(f\"Best model path according to precision: {best_model_path}\")\n",
    "    print(f\"Full metrics: {best_step_metrics}\")\n",
    "    \n",
    "    eval_results = evaluation.run_eval(data=eval_lines, \n",
    "                        model_path=best_model_path, \n",
    "                        tokenizer_name=tokenizer.name_or_path, \n",
    "                        verbose=False, \n",
    "                        lang=eval_data_lang)\n",
    "    \n",
    "\n",
    "    # We move the best state dir name to \"best\"\n",
    "    new_best_path = f\"results_{name_of_model}/epoch{num_train_epochs}_bs{batch_size}/best\"\n",
    "    try:\n",
    "        #os.rmdir(new_best_path)\n",
    "        shutil.rmtree(new_best_path)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    os.rename(best_model_path, new_best_path)\n",
    "    \n",
    "    #with open(f\"{new_best_path}/model_name\", \"w\") as model_name:\n",
    "    #    model_name.write(modelName)\n",
    "\n",
    "    with open(f\"{new_best_path}/eval.txt\", \"w\") as evaluation_results:\n",
    "        evaluation_results.write(eval_results)\n",
    "\n",
    "    with open(f\"{new_best_path}/metrics.json\", \"w\") as metrics:\n",
    "        json.dump(best_step_metrics, metrics)\n",
    "    \n",
    "    print(f\"\\n\\nBest model can be found at : {new_best_path} \")\n",
    "    print(f\"You should remove the following directories by using `rm -r results_{name_of_model}/epoch{num_train_epochs}_bs{batch_size}/checkpoint-*`\")\n",
    "\n",
    "    # functions returns best model_path\n",
    "    return new_best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab5d9d75-163a-4e61-a1bf-ef8a32417db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-french-europeana-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lucenceing/Documents/atelier_biblissima_aquilign/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucenceing/Documents/atelier_biblissima_aquilign/venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 02:47, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accurracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.098100</td>\n",
       "      <td>0.041685</td>\n",
       "      <td>{'accuracy': 0.9857142857142858}</td>\n",
       "      <td>[0.9889434889434889, 0.8842105263157894, 1.0]</td>\n",
       "      <td>[0.9865196078431373, 0.9130434782608695, 0.9979674796747967]</td>\n",
       "      <td>[0.9877300613496932, 0.8983957219251337, 0.9989827060020345]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.034185</td>\n",
       "      <td>{'accuracy': 0.9907142857142858}</td>\n",
       "      <td>[0.9926289926289926, 0.9263157894736842, 1.0]</td>\n",
       "      <td>[0.9914110429447853, 0.946236559139785, 0.9979674796747967]</td>\n",
       "      <td>[0.9920196439533456, 0.9361702127659575, 0.9989827060020345]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting eval\n",
      "Eval finished\n",
      "Starting eval\n",
      "Eval finished\n",
      "End of training\n",
      "results_bert-base-french-europeana-cased/epoch2_bs8/checkpoint-250\n",
      "Evaluation.\n",
      "Best step according to precision: 250\n",
      "Best model path according to precision: results_bert-base-french-europeana-cased/epoch2_bs8/checkpoint-250\n",
      "Full metrics: {'loss': 0.0231, 'grad_norm': 0.5186708569526672, 'learning_rate': 0.0, 'epoch': 2.0, 'step': 250, 'eval_loss': 0.0341854952275753, 'eval_accurracy': {'accuracy': 0.9907142857142858}, 'eval_recall': [0.9926289926289926, 0.9263157894736842, 1.0], 'eval_precision': [0.9914110429447853, 0.946236559139785, 0.9979674796747967], 'eval_f1': [0.9920196439533456, 0.9361702127659575, 0.9989827060020345], 'eval_runtime': 4.42, 'eval_samples_per_second': 11.312, 'eval_steps_per_second': 1.584, 'train_runtime': 169.2471, 'train_samples_per_second': 11.817, 'train_steps_per_second': 1.477, 'total_flos': 30620990520000.0, 'train_loss': 0.06056657981872558}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucenceing/Documents/atelier_biblissima_aquilign/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing syntactic tokenization evaluation\n",
      "(0.9547619047619048, [0.9417637271214643, 0.8188405797101449, 1.0], [0.9783923941227312, 0.6174863387978142, 1.0], [0.9597286986011022, 0.7040498442367601, 1.0])\n",
      "Performing bert-based tokenization evaluation\n",
      "|           | Synt (None, Delim.)                           | Bert (None, Delim., Pad.)                     |\n",
      "|-----------+-----------------------------------------------+-----------------------------------------------|\n",
      "| Accuracy  | 0.9547619047619048                            | 0.9869565217391304                            |\n",
      "| Precision | [0.9417637271214643, 0.8188405797101449, 1.0] | [0.9895742832319722, 0.9047619047619048, 1.0] |\n",
      "| Recall    | [0.9783923941227312, 0.6174863387978142, 1.0] | [0.9844425237683665, 0.9344262295081968, 1.0] |\n",
      "| F1-score  | [0.9597286986011022, 0.7040498442367601, 1.0] | [0.987001733102253, 0.9193548387096774, 1.0]  |\n",
      "\n",
      "\n",
      "Best model can be found at : results_bert-base-french-europeana-cased/epoch2_bs8/best \n",
      "You should remove the following directories by using `rm -r results_bert-base-french-europeana-cased/epoch2_bs8/checkpoint-*`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'results_bert-base-french-europeana-cased/epoch2_bs8/best'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_trainer('dbmdz/bert-base-french-europeana-cased', 'dbmdz/bert-base-french-europeana-cased', 'data_to_segmenter/fr/randomSentencesComplete-gf.txt', 'data_to_segmenter/fr/randomSentencesEvalComplete-gf.txt', 'data_to_segmenter/fr/randomSentencesfrancais-pourtest-complete-gf.txt', 2, 8, 50, keep_punct=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
