{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e710e2-b398-47ba-a89c-6124cfb65b76",
   "metadata": {},
   "source": [
    "# Step 1. Training a segmentation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681c4bb-d184-4c82-aeeb-3f1874372305",
   "metadata": {},
   "source": [
    "Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4b821-dad7-4a45-91de-8346ae4d6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import sys\n",
    "from transformers import BertTokenizer, Trainer, TrainingArguments, AutoModelForTokenClassification, set_seed\n",
    "import aquilign.preproc.tok_trainer_functions as trainer_functions\n",
    "import aquilign.preproc.eval as evaluation\n",
    "import aquilign.preproc.utils as utils\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import argparse\n",
    "#shutil usefull for deleting not empty directories \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537553f-ad79-4d32-85d0-fab2d57b1836",
   "metadata": {},
   "source": [
    "The script is used to train an automatic text segmentation model. Three files must be supplied, all in the specified format (each token to be identified as segmenting text must be preceded by a ‘£’, character that has been chosen as a delimiter): one file containing training data, one containing dev data, and one containing test data. The files must be in a folder containing the ISO code of the language in which they are written (this code is retrieved when the models are evaluated).\n",
    "\n",
    "The best model is saved at the end of training. Evaluation is based on both loss and more traditional evaluation metrics. In our script, accuracy is the most important metric and it has a more import weight when choosing the best model.\n",
    "Evaluation also involves the comparative evaluation of a regex-based segmentation. It is therefore necessary to fill the delimiters.json file with example regexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77799585-3a43-4343-bdab-24f61cc08628",
   "metadata": {},
   "outputs": [],
   "source": [
    "## command line usage : python tok_trainer.py model_name tok_name train_file.txt dev_file.txt num_train_epochs batch_size logging_steps\n",
    "## where :\n",
    "# model_name is the full name of the model (same name for model and tokenizer or not)\n",
    "# tok_name is the full name of the tokenizer (can be the same)\n",
    "# train_file.txt is the file with the sentences and words of interest are identified  (words are identified with £ in the line)\n",
    "# which will be used for training\n",
    "## ex. : uoulentiers £mais il nen est pas encor temps. £Certes fait elle\n",
    "# dev_file.txt is the file with the sentences and words of interest which will be used for eval\n",
    "# num_train_epochs : the number of epochs we want to train (ex : 10)\n",
    "# batch_size : the batch size (ex : 8)\n",
    "# logging_steps : the number of logging steps (ex : 50)\n",
    "\n",
    "## was changed : if you want to fine-tune a model, we need to have two different names for model_name and tok_name (can also be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455a345-a250-4340-baee-d5a9cd9b49b0",
   "metadata": {},
   "source": [
    "The training_trainer function takes several arguments:\n",
    "- model_name: the name of the AutoModelForTokenClassification model\n",
    "- tok_name: the name of the BertTokenizer model\n",
    "(these two names can also be the same, if you're not fine-tuning a specific model)\n",
    "- train_dataset: the path of the training data file\n",
    "- dev_dataset: the path of the dev data file\n",
    "- eval_dataset: the path of the test data file\n",
    "- num_train_epochs: the number of training epochs (min. 2)\n",
    "- batch_size\n",
    "- logging_steps\n",
    "  \n",
    "And, in addition, an argument allowing you to say whether or not you want to keep punctuation as a segmentation aid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba8ae12-6c46-49be-a526-c53b6d0ede10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_trainer(model_name, tok_name, train_dataset, dev_dataset, eval_dataset, num_train_epochs, batch_size, logging_steps, keep_punct=True):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=3)\n",
    "    tokenizer = BertTokenizer.from_pretrained(tok_name, max_length=10)\n",
    "    \n",
    "    with open(train_dataset, \"r\") as train_file:\n",
    "        train_lines = [item.replace(\"\\n\", \"\") for item in train_file.readlines()]\n",
    "        if keep_punct is False:\n",
    "            train_lines = [utils.remove_punctuation(line) for line in train_lines]\n",
    "        \n",
    "    with open(dev_dataset, \"r\") as dev_file:\n",
    "        dev_lines = [item.replace(\"\\n\", \"\") for item in dev_file.readlines()]\n",
    "        if keep_punct is False:\n",
    "            dev_lines = [utils.remove_punctuation(line) for line in dev_lines]\n",
    "        \n",
    "    with open(eval_dataset, \"r\") as eval_files:\n",
    "        eval_lines = [item.replace(\"\\n\", \"\") for item in eval_files.readlines()]\n",
    "    eval_data_lang = eval_dataset.split(\"/\")[-2]\n",
    "    \n",
    "    # Train corpus\n",
    "    train_texts_and_labels = utils.convertToSubWordsSentencesAndLabels(train_lines, tokenizer=tokenizer, delimiter=\"£\")\n",
    "    train_dataset = trainer_functions.SentenceBoundaryDataset(train_texts_and_labels, tokenizer)\n",
    "    \n",
    "    # Dev corpus\n",
    "    dev_texts_and_labels = utils.convertToSubWordsSentencesAndLabels(dev_lines, tokenizer=tokenizer, delimiter=\"£\")\n",
    "    dev_dataset = trainer_functions.SentenceBoundaryDataset(dev_texts_and_labels, tokenizer)\n",
    "\n",
    "    if '/' in model_name:\n",
    "        name_of_model = re.split('/', model_name)[1]\n",
    "    else:\n",
    "        name_of_model = model_name\n",
    "\n",
    "    # training arguments\n",
    "    # num train epochs, logging_steps and batch_size should be provided\n",
    "    # evaluation is done by epoch and the best model of each one is stored in a folder \"results_+name\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"results_{name_of_model}/epoch{num_train_epochs}_bs{batch_size}\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        logging_steps=logging_steps,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        dataloader_num_workers=8,\n",
    "        dataloader_prefetch_factor=4,\n",
    "        # ajout pour résoudre pb : save_safetensors= False et bf16=False\n",
    "        bf16=False,    \n",
    "        save_safetensors=False,\n",
    "        #modif : cpu\n",
    "        use_cpu=True,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True\n",
    "        # best model is evaluated on loss\n",
    "    )\n",
    "\n",
    "    # define the trainer : model, training args, datasets and the specific compute_metrics defined in functions file\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        compute_metrics=trainer_functions.compute_metrics\n",
    "    )\n",
    "\n",
    "    # fine-tune the model\n",
    "    print(\"Starting training\")\n",
    "    trainer.train()\n",
    "    print(\"End of training\")\n",
    "\n",
    "    # get the best model path\n",
    "    best_model_path = trainer.state.best_model_checkpoint\n",
    "    print(best_model_path)\n",
    "    print(f\"Evaluation.\")\n",
    "    \n",
    "    \n",
    "    # print the whole log_history with the compute metrics\n",
    "    best_precision_step, best_step_metrics = utils.get_best_step(trainer.state.log_history)\n",
    "    best_model_path = f\"results_{name_of_model}/epoch{num_train_epochs}_bs{batch_size}/checkpoint-{best_precision_step}\"\n",
    "    print(f\"Best model path according to precision: {best_model_path}\")\n",
    "    print(f\"Full metrics: {best_step_metrics}\")\n",
    "    \n",
    "    eval_results = evaluation.run_eval(data=eval_lines, \n",
    "                        model_path=best_model_path, \n",
    "                        tokenizer_name=tokenizer.name_or_path, \n",
    "                        verbose=False, \n",
    "                        lang=eval_data_lang)\n",
    "    \n",
    "\n",
    "    # We move the best state dir name to \"best\"\n",
    "    new_best_path = f\"results_{name_of_model}/epoch{num_train_epochs}_bs{batch_size}/best\"\n",
    "    try:\n",
    "        #os.rmdir(new_best_path)\n",
    "        shutil.rmtree(new_best_path)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    os.rename(best_model_path, new_best_path)\n",
    "    \n",
    "    #with open(f\"{new_best_path}/model_name\", \"w\") as model_name:\n",
    "    #    model_name.write(modelName)\n",
    "\n",
    "    with open(f\"{new_best_path}/eval.txt\", \"w\") as evaluation_results:\n",
    "        evaluation_results.write(eval_results)\n",
    "\n",
    "    with open(f\"{new_best_path}/metrics.json\", \"w\") as metrics:\n",
    "        json.dump(best_step_metrics, metrics)\n",
    "    \n",
    "    print(f\"\\n\\nBest model can be found at : {new_best_path} \")\n",
    "    print(f\"You should remove the following directories by using `rm -r results_{name_of_model}/epoch{num_train_epochs}_bs{batch_size}/checkpoint-*`\")\n",
    "\n",
    "    # functions returns best model_path\n",
    "    return new_best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d9d75-163a-4e61-a1bf-ef8a32417db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_trainer('dbmdz/bert-base-french-europeana-cased', 'dbmdz/bert-base-french-europeana-cased', 'data/data_to_segmenter/fr/randomSentencesComplete-gf.txt', 'data/data_to_segmenter/fr/randomSentencesEvalComplete-gf.txt', 'data/data_to_segmenter/fr/randomSentencesfrancais-pourtest-complete-gf.txt', 2, 8, 50, keep_punct=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10_env",
   "language": "python",
   "name": "3.10_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
